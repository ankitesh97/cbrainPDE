{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sympy\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "import pickle\n",
    "import configparser\n",
    "import yaml\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SymNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, n_deriv_channel, deriv_channel_names=None, normalization_weight=None):\n",
    "        '''\n",
    "        Input:\n",
    "            n_channel = Number of derivatives using (u,ux,vx,vxx)\n",
    "        '''\n",
    "        super(SymNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_deriv_channel = n_deriv_channel\n",
    "        if deriv_channel_names is None:\n",
    "            deriv_channel_names = list('u_'+str(i) for i in range(self.n_deriv_channel))\n",
    "        self.deriv_channel_names = deriv_channel_names\n",
    "        layers = []\n",
    "        for k in range(n_hidden):\n",
    "            module = torch.nn.Linear(n_deriv_channel+k,2)\n",
    "            self.add_module('layer'+str(k), module)\n",
    "            layers.append(self.__getattr__('layer'+str(k)))\n",
    "        module = torch.nn.Linear(n_deriv_channel+n_hidden, 1)\n",
    "        self.add_module('layer_final', module)\n",
    "        layers.append(self.__getattr__('layer_final'))\n",
    "        self.layers = tuple(layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "            inputs tensor be of shape (batch_size x X_dim x n_derivatives)\n",
    "            output shape batch_size x X_dim\n",
    "        '''\n",
    "        outputs = inputs.type(torch.FloatTensor)\n",
    "        for k in range(self.n_hidden):\n",
    "            o = self.layers[k](outputs)\n",
    "            outputs = torch.cat([outputs,o[...,:1]*o[...,1:]], dim=-1)\n",
    "        outputs = self.layers[-1](outputs)\n",
    "        \n",
    "        return outputs[...,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _cast2symbol(self,layer):\n",
    "        weight,bias = layer.weight.data.cpu().numpy(), \\\n",
    "                    layer.bias.data.cpu().numpy()\n",
    "        weight,bias = sympy.Matrix(weight),sympy.Matrix(bias)\n",
    "        return weight,bias\n",
    "\n",
    "    def _sympychop(self,o, calprec):\n",
    "        for i in range(o.shape[0]):\n",
    "            cdict = o[i].expand().as_coefficients_dict()  \n",
    "            o_i = 0\n",
    "            for k,v in cdict.items():\n",
    "                if abs(v)>0.1**calprec:\n",
    "                    o_i = o_i+k*v\n",
    "            o[i] = o_i\n",
    "        return o\n",
    "\n",
    "    def getEquation(self,calprec=6):\n",
    "        ## assume symnet model\n",
    "\n",
    "        deriv_channels = sympy.symbols(self.deriv_channel_names)\n",
    "        deriv_channels = sympy.Matrix([deriv_channels,])\n",
    "        for i in range(self.n_hidden):\n",
    "            weight,bias = self._cast2symbol(self.layers[i])\n",
    "            o = weight*deriv_channels.transpose()+bias\n",
    "            o = self._sympychop(o, calprec) #ignores very low params terms\n",
    "            deriv_channels = list(deriv_channels)+[o[0]*o[1],]\n",
    "            deriv_channels = sympy.Matrix([deriv_channels,])\n",
    "\n",
    "        weight,bias = self._cast2symbol(self.layers[-1])\n",
    "        o = (weight*deriv_channels.transpose()+bias)\n",
    "        o = _sympychop(o,calprec)\n",
    "\n",
    "        return o[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SymNet(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [[[1,2,3,4],[1,2,3,4],[1,2,3,4]]]\n",
    "inp = torch.from_numpy(np.array(inp)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6280, -0.6280, -0.6280]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.38415 u_{0} - 0.196855 u_{1} - 0.38974 u_{2} + 0.378879$"
      ],
      "text/plain": [
       "0.38415*u_0 - 0.196855*u_1 - 0.38974*u_2 + 0.378879"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.getEquation(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FD1D(torch.nn.Module):\n",
    "    '''\n",
    "        Finite Differences scheme for 1D dependency\n",
    "        acc_order list of acc_order for diff_order\n",
    "        kernel size should be greater than or equal to the max_diff_order (otherwise error will be thrown)\n",
    "        diff_order starts from 0,1,... \n",
    "        \n",
    "        Basically this class initializes one kernel of the specified parameters\n",
    "    '''\n",
    "    def __init__(self,dx, kernel_size, diff_order,acc_order):\n",
    "        super(FD1D, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.kernel_size = kernel_size\n",
    "        self.diff_order = diff_order\n",
    "        self.acc_order = acc_order\n",
    "\n",
    "        self.kernel = getKernelTorch(diff_order,acc_order,dim=kernel_size,scheme='central')/(dx**diff_order)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        '''\n",
    "            Process:\n",
    "            Need to pad the input and then appy conv1D\n",
    "            input shape can be batch_size x n_channels x x_dim\n",
    "        '''\n",
    "        inp_padded = padInputTorch(inputs,self.diff_order,self.acc_order,dim=self.kernel_size) #batch_size x n_channels x (x_dim+padded)\n",
    "#         print(inp_padded)\n",
    "        conv = F.conv1d(inp_padded,self.kernel)\n",
    "        return conv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1,1,40)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.1643, -3.3742,  1.4160,  0.6854, -0.1082, -1.6104,  1.3949,\n",
       "           1.3783, -0.8669, -2.5061,  1.4144,  1.5445, -2.0005,  2.8164,\n",
       "          -4.1300,  2.9963, -1.1669, -0.5869,  1.4000,  0.7138, -0.4725,\n",
       "          -1.6408,  0.4569, -1.0700,  4.9659, -3.9201, -0.6370,  2.2561,\n",
       "          -0.1850, -3.4091,  2.7100,  0.1704, -0.0895,  0.8708, -0.1929,\n",
       "          -2.8485,  2.6325, -2.1210,  1.8284,  5.7779]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FD1D(1,5,2,2) #fix nans in accuracy order > 2\n",
    "fd(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDE Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdeNet(torch.nn.Module):\n",
    "    def __init__(self,dt, dx, kernel_size, max_diff_order, n_channel,channel_names,acc_order=2,n_hidden=2):\n",
    "        '''\n",
    "        Input:\n",
    "        '''\n",
    "        super(PdeNet, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dt = dt\n",
    "        self.kernel_size = kernel_size\n",
    "        self.max_diff_order = max_diff_order\n",
    "        self.n_channel = n_channel\n",
    "        self.channel_names = channel_names\n",
    "        self.n_hidden = n_hidden\n",
    "                        \n",
    "        if not np.iterable(acc_order):\n",
    "            acc_order = [acc_order,]*(self.max_diff_order+1)\n",
    "            \n",
    "        self.acc_order = acc_order\n",
    "        \n",
    "        #conv operation\n",
    "        for i in range(max_diff_order+1):\n",
    "            kernel = FD1D(dx,kernel_size,i,acc_order[i])\n",
    "            self.add_module('fd'+str(i), kernel) #finite difference of order\n",
    "            \n",
    "        #symnet \n",
    "        c = channel_names.split(',')\n",
    "        derivative_channels = []\n",
    "        for ch in c:\n",
    "            for k in range(max_diff_order+1):\n",
    "                derivative_channels.append(ch+'_'+str(k))\n",
    "        self.derivative_channels = derivative_channels \n",
    "        self.add_module(\"symnet\",SymNet(n_hidden,len(derivative_channels), deriv_channel_names=derivative_channels))\n",
    "    \n",
    "    def multistep(self,inputs,step_num):\n",
    "        #pass it throught the kernels then the symmnet to \n",
    "        '''\n",
    "        Takes multistep through the whole PDE Net.\n",
    "        '''\n",
    "        u = inputs\n",
    "        for i in range(step_num):\n",
    "            uadd = self.RightHandItems(u)#will take a dt step from u using the network\n",
    "            u = u + self.dt*uadd\n",
    "        return u\n",
    "    \n",
    "    def RightHandItems(self,u):\n",
    "        \n",
    "        #convolve the u with the derivative kernals to get the different derivatives \n",
    "        #batch_size x n_channels x X_dim\n",
    "        u_derives = []\n",
    "        for i in range(self.max_diff_order+1):\n",
    "            fd_obj = self.__getattr__('fd'+str(i))\n",
    "            u_deriv_order_i = fd_obj(u)\n",
    "            \n",
    "            u_derives.append(u_deriv_order_i)\n",
    "            \n",
    "        u = torch.cat(u_derives, dim=1) #batch_size x n_derivatives x X_dim \n",
    "        #symnet_output = (batch_size x X_dim x n_derivatives)\n",
    "        symnet = self.__getattr__('symnet')\n",
    "        u_symnet = symnet(u.permute(0,2,1)) #batch_size x X_dim\n",
    "        u_out = u_symnet.unsqueeze_(1)\n",
    "        return u_out\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs,step_num):\n",
    "        '''\n",
    "            inputs of shape batch_size x n_channels(1 for our case) x X_dim\n",
    "            step_nums = number of dt blocks to calculate the inputs for\n",
    "        '''\n",
    "        return self.multistep(inputs,step_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PdeNet(\n",
      "  (fd0): FD1D()\n",
      "  (fd1): FD1D()\n",
      "  (fd2): FD1D()\n",
      "  (symnet): SymNet(\n",
      "    (layer0): Linear(in_features=3, out_features=2, bias=True)\n",
      "    (layer1): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (layer_final): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = PdeNet(dt=0.01,dx=0.1,kernel_size=5,max_diff_order=2,n_channel=1,channel_names='u')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run it for 1 step from start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1,1,40)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0171e+01, -2.4517e-01,  8.6392e-01, -2.7246e-01,  3.5923e-01,\n",
      "          -9.0346e-01,  7.2720e+00,  1.3810e+02,  5.8666e+00, -5.5084e-01,\n",
      "           4.7074e-01,  2.1184e-01,  3.1020e+00, -1.2126e+00,  1.2576e+00,\n",
      "          -4.8390e-01,  6.9350e-01,  8.6608e-01, -1.5373e-01,  4.9069e+01,\n",
      "           3.5265e+01, -1.2972e+00,  1.5328e+00,  1.4518e+00, -2.0000e-01,\n",
      "           8.5266e-01,  2.7885e-01, -9.9466e-01,  1.0931e+00, -5.1715e-01,\n",
      "           1.5044e+01, -6.7751e-01, -2.0942e-01, -1.7663e-01, -4.8952e-02,\n",
      "           8.6972e-01,  7.7066e-01,  4.3546e-01,  1.0066e+00,  2.3007e+00]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = net(inp,step_num=1)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('symnet.layer0.weight', tensor([[ 0.2226,  0.2604,  0.1286],\n",
       "                      [ 0.2489,  0.0043, -0.1339]])),\n",
       "             ('symnet.layer0.bias', tensor([-0.4999, -0.3734])),\n",
       "             ('symnet.layer1.weight',\n",
       "              tensor([[-0.3535, -0.4990,  0.1757, -0.2431],\n",
       "                      [-0.3812, -0.0333, -0.1424,  0.4230]])),\n",
       "             ('symnet.layer1.bias', tensor([-0.2737, -0.4254])),\n",
       "             ('symnet.layer_final.weight',\n",
       "              tensor([[ 0.0192,  0.0190, -0.3789,  0.2718, -0.0036]])),\n",
       "             ('symnet.layer_final.bias', tensor([-0.4418]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 2 losses, one is data loss and the other is symnet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symnetRegularizeLoss(model):\n",
    "    loss = 0\n",
    "    s = 1e-2\n",
    "    for p in model.symnet.parameters():\n",
    "        p = p.abs()\n",
    "        loss = loss+((p<s).to(p)*0.5/s*p**2).sum()+((p>=s).to(p)*(p-s/2)).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global names are all the parameters\n",
    "def modelLoss(model,u_obs,config,block):\n",
    "    '''\n",
    "        Returns the loss value for so that it can be given to an optimizer\n",
    "        Inputs:\n",
    "            u_obs (batch_size x n_channels x X_dim)\n",
    "            blocks is stepnum\n",
    "    '''\n",
    "    sparsity = config['sparsity']\n",
    "    \n",
    "    if block==0: #warmup\n",
    "        sparsity = 0\n",
    "    step_num = block if block>=1 else 1\n",
    "    dt = config['dt']\n",
    "    data_loss = 0\n",
    "    symnet_loss = symnetRegularizeLoss(model)\n",
    "    ut = u_obs[0]\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    for steps in range(1,step_num+1):\n",
    "        ut_next_predicted = model(ut,step_num=1) #take one step from this point\n",
    "        data_loss += (mse_loss(ut_next_predicted,u_obs[steps])/dt**2)/step_num\n",
    "        ut = ut_next_predicted\n",
    "\n",
    "    loss = data_loss+stepnum*sparsity*symnet_loss\n",
    "    if torch.isnan(loss):\n",
    "        raise \"Loss Nan\"\n",
    "        loss = (torch.ones(1,requires_grad=True)/torch.zeros(1)).to(loss)\n",
    "    return loss,data_loss,symnet_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "##modify channel names and length\n",
    "def setenv(config): #return model and datamodel\n",
    "    model = PdeNet(config['dt'],config['dx'],config['kernel_size'],config['max_diff_order']\\\n",
    "                   ,1,config['channel_names'],config['acc_order'],config['n_hidden_layers'])\n",
    "    \n",
    "    #data model \n",
    "    if 'Diffusion' in config['name']:\n",
    "    #data model \n",
    "        data_model = DiffusionDataTrign(config['name'],config['Nt'],\\\n",
    "                                   config['dt'],config['dx'],config['viscosity'],batch_size=config['batch_size'],\\\n",
    "                                  time_scheme=config['data_timescheme'],acc_order=config['acc_order'])        \n",
    "    if 'Burgers' in config['name']:\n",
    "        data_model = BurgersEqnTrign(config['name'],config['Nt'],\\\n",
    "                               config['dt'],config['dx'],config['viscosity'],batch_size=config['batch_size'],\\\n",
    "                              time_scheme=config['data_timescheme'],acc_order=config['acc_order'])\n",
    "    #possible some callbacks\n",
    "    callbacks = None\n",
    "    return model,data_model,callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Diffusion Eqn',\n",
       " 'dt': 0.01,\n",
       " 'dx': 0.1,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 2,\n",
       " 'acc_order': 2,\n",
       " 'n_hidden_layers': 2,\n",
       " 'dataname': 'Diffusion',\n",
       " 'viscosity': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'channel_names': 'u',\n",
       " 'data_timescheme': 'rk4',\n",
       " 'data_dir': '/sdsd/dsds/sdsd',\n",
       " 'Nt': 100,\n",
       " 'Nx': 32,\n",
       " 'sigma': 1,\n",
       " 'sparsity': 0.05,\n",
       " 'epochs': 500,\n",
       " 'results_dir': '/comet/results/',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.005}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = config['blocks']\n",
    "dt = config['dt']\n",
    "dx = config['dx']\n",
    "epochs = config['epochs']\n",
    "lr = config['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n",
      "[PRINT] Epoch: 0, Loss: 6.244, Data Loss: 6.244, Symnet Regularize: 5.285\n",
      "[PRINT] Epoch: 10, Loss: 3.458, Data Loss: 3.458, Symnet Regularize: 5.422\n",
      "[PRINT] Epoch: 20, Loss: 1.961, Data Loss: 1.961, Symnet Regularize: 5.304\n",
      "[PRINT] Epoch: 30, Loss: 1.124, Data Loss: 1.124, Symnet Regularize: 5.233\n",
      "[PRINT] Epoch: 40, Loss: 0.693, Data Loss: 0.693, Symnet Regularize: 5.135\n",
      "[PRINT] Epoch: 50, Loss: 0.453, Data Loss: 0.453, Symnet Regularize: 5.091\n",
      "[PRINT] Epoch: 60, Loss: 0.310, Data Loss: 0.310, Symnet Regularize: 5.075\n",
      "[PRINT] Epoch: 70, Loss: 0.218, Data Loss: 0.218, Symnet Regularize: 5.062\n",
      "[PRINT] Epoch: 80, Loss: 0.152, Data Loss: 0.152, Symnet Regularize: 5.068\n",
      "[PRINT] Epoch: 90, Loss: 0.106, Data Loss: 0.106, Symnet Regularize: 5.079\n",
      "[PRINT] Epoch: 100, Loss: 0.073, Data Loss: 0.073, Symnet Regularize: 5.108\n",
      "[PRINT] Epoch: 110, Loss: 0.051, Data Loss: 0.051, Symnet Regularize: 5.151\n",
      "[PRINT] Epoch: 120, Loss: 0.035, Data Loss: 0.035, Symnet Regularize: 5.184\n",
      "[PRINT] Epoch: 130, Loss: 0.025, Data Loss: 0.025, Symnet Regularize: 5.208\n",
      "[PRINT] Epoch: 140, Loss: 0.018, Data Loss: 0.018, Symnet Regularize: 5.223\n",
      "[PRINT] Epoch: 150, Loss: 0.013, Data Loss: 0.013, Symnet Regularize: 5.231\n",
      "[PRINT] Epoch: 160, Loss: 0.010, Data Loss: 0.010, Symnet Regularize: 5.233\n",
      "[PRINT] Epoch: 170, Loss: 0.008, Data Loss: 0.008, Symnet Regularize: 5.231\n",
      "[PRINT] Epoch: 180, Loss: 0.006, Data Loss: 0.006, Symnet Regularize: 5.227\n",
      "[PRINT] Epoch: 190, Loss: 0.005, Data Loss: 0.005, Symnet Regularize: 5.220\n",
      "[PRINT] Epoch: 200, Loss: 0.005, Data Loss: 0.005, Symnet Regularize: 5.212\n",
      "[PRINT] Epoch: 210, Loss: 0.004, Data Loss: 0.004, Symnet Regularize: 5.204\n",
      "[PRINT] Epoch: 220, Loss: 0.003, Data Loss: 0.003, Symnet Regularize: 5.195\n",
      "[PRINT] Epoch: 230, Loss: 0.003, Data Loss: 0.003, Symnet Regularize: 5.187\n",
      "[PRINT] Epoch: 240, Loss: 0.003, Data Loss: 0.003, Symnet Regularize: 5.178\n",
      "[PRINT] Epoch: 250, Loss: 0.003, Data Loss: 0.003, Symnet Regularize: 5.169\n",
      "[PRINT] Epoch: 260, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.161\n",
      "[PRINT] Epoch: 270, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.153\n",
      "[PRINT] Epoch: 280, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.145\n",
      "[PRINT] Epoch: 290, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.137\n",
      "[PRINT] Epoch: 300, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.130\n",
      "[PRINT] Epoch: 310, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.123\n",
      "[PRINT] Epoch: 320, Loss: 0.002, Data Loss: 0.002, Symnet Regularize: 5.116\n",
      "[PRINT] Epoch: 330, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.111\n",
      "[PRINT] Epoch: 340, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.106\n",
      "[PRINT] Epoch: 350, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.102\n",
      "[PRINT] Epoch: 360, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.100\n",
      "[PRINT] Epoch: 370, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.098\n",
      "[PRINT] Epoch: 380, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.096\n",
      "[PRINT] Epoch: 390, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.095\n",
      "[PRINT] Epoch: 400, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.095\n",
      "[PRINT] Epoch: 410, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.094\n",
      "[PRINT] Epoch: 420, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.094\n",
      "[PRINT] Epoch: 430, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.093\n",
      "[PRINT] Epoch: 440, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.092\n",
      "[PRINT] Epoch: 450, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.092\n",
      "[PRINT] Epoch: 460, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.091\n",
      "[PRINT] Epoch: 470, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.091\n",
      "[PRINT] Epoch: 480, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.090\n",
      "[PRINT] Epoch: 490, Loss: 0.001, Data Loss: 0.001, Symnet Regularize: 5.090\n",
      "[PRINT] block: 1\n",
      "[PRINT] Epoch: 0, Loss: 0.255, Data Loss: 0.001, Symnet Regularize: 5.090\n",
      "[PRINT] Epoch: 10, Loss: 0.246, Data Loss: 0.001, Symnet Regularize: 4.887\n",
      "[PRINT] Epoch: 20, Loss: 0.228, Data Loss: 0.002, Symnet Regularize: 4.526\n",
      "[PRINT] Epoch: 30, Loss: 0.212, Data Loss: 0.003, Symnet Regularize: 4.171\n",
      "[PRINT] Epoch: 40, Loss: 0.198, Data Loss: 0.003, Symnet Regularize: 3.899\n",
      "[PRINT] Epoch: 50, Loss: 0.184, Data Loss: 0.002, Symnet Regularize: 3.637\n",
      "[PRINT] Epoch: 60, Loss: 0.171, Data Loss: 0.002, Symnet Regularize: 3.384\n",
      "[PRINT] Epoch: 70, Loss: 0.158, Data Loss: 0.002, Symnet Regularize: 3.127\n",
      "[PRINT] Epoch: 80, Loss: 0.147, Data Loss: 0.001, Symnet Regularize: 2.908\n",
      "[PRINT] Epoch: 90, Loss: 0.139, Data Loss: 0.001, Symnet Regularize: 2.758\n",
      "[PRINT] Epoch: 100, Loss: 0.132, Data Loss: 0.001, Symnet Regularize: 2.614\n",
      "[PRINT] Epoch: 110, Loss: 0.124, Data Loss: 0.001, Symnet Regularize: 2.469\n",
      "[PRINT] Epoch: 120, Loss: 0.117, Data Loss: 0.001, Symnet Regularize: 2.323\n",
      "[PRINT] Epoch: 130, Loss: 0.110, Data Loss: 0.001, Symnet Regularize: 2.182\n",
      "[PRINT] Epoch: 140, Loss: 0.103, Data Loss: 0.001, Symnet Regularize: 2.041\n",
      "[PRINT] Epoch: 150, Loss: 0.097, Data Loss: 0.001, Symnet Regularize: 1.926\n",
      "[PRINT] Epoch: 160, Loss: 0.093, Data Loss: 0.001, Symnet Regularize: 1.843\n",
      "[PRINT] Epoch: 170, Loss: 0.089, Data Loss: 0.001, Symnet Regularize: 1.766\n",
      "[PRINT] Epoch: 180, Loss: 0.086, Data Loss: 0.001, Symnet Regularize: 1.701\n",
      "[PRINT] Epoch: 190, Loss: 0.083, Data Loss: 0.001, Symnet Regularize: 1.645\n",
      "[PRINT] Epoch: 200, Loss: 0.081, Data Loss: 0.001, Symnet Regularize: 1.605\n",
      "[PRINT] Epoch: 210, Loss: 0.079, Data Loss: 0.001, Symnet Regularize: 1.561\n",
      "[PRINT] Epoch: 220, Loss: 0.077, Data Loss: 0.001, Symnet Regularize: 1.518\n",
      "[PRINT] Epoch: 230, Loss: 0.075, Data Loss: 0.001, Symnet Regularize: 1.474\n",
      "[PRINT] Epoch: 240, Loss: 0.072, Data Loss: 0.001, Symnet Regularize: 1.430\n",
      "[PRINT] Epoch: 250, Loss: 0.071, Data Loss: 0.001, Symnet Regularize: 1.394\n",
      "[PRINT] Epoch: 260, Loss: 0.069, Data Loss: 0.001, Symnet Regularize: 1.365\n",
      "[PRINT] Epoch: 270, Loss: 0.068, Data Loss: 0.001, Symnet Regularize: 1.333\n",
      "[PRINT] Epoch: 280, Loss: 0.066, Data Loss: 0.001, Symnet Regularize: 1.302\n",
      "[PRINT] Epoch: 290, Loss: 0.065, Data Loss: 0.001, Symnet Regularize: 1.274\n",
      "[PRINT] Epoch: 300, Loss: 0.063, Data Loss: 0.001, Symnet Regularize: 1.247\n",
      "[PRINT] Epoch: 310, Loss: 0.062, Data Loss: 0.001, Symnet Regularize: 1.219\n",
      "[PRINT] Epoch: 320, Loss: 0.060, Data Loss: 0.001, Symnet Regularize: 1.191\n",
      "[PRINT] Epoch: 330, Loss: 0.059, Data Loss: 0.001, Symnet Regularize: 1.161\n",
      "[PRINT] Epoch: 340, Loss: 0.057, Data Loss: 0.001, Symnet Regularize: 1.131\n",
      "[PRINT] Epoch: 350, Loss: 0.056, Data Loss: 0.001, Symnet Regularize: 1.100\n",
      "[PRINT] Epoch: 360, Loss: 0.054, Data Loss: 0.001, Symnet Regularize: 1.068\n",
      "[PRINT] Epoch: 370, Loss: 0.053, Data Loss: 0.001, Symnet Regularize: 1.035\n",
      "[PRINT] Epoch: 380, Loss: 0.051, Data Loss: 0.001, Symnet Regularize: 1.001\n",
      "[PRINT] Epoch: 390, Loss: 0.049, Data Loss: 0.001, Symnet Regularize: 0.967\n",
      "[PRINT] Epoch: 400, Loss: 0.048, Data Loss: 0.001, Symnet Regularize: 0.935\n",
      "[PRINT] Epoch: 410, Loss: 0.046, Data Loss: 0.001, Symnet Regularize: 0.902\n",
      "[PRINT] Epoch: 420, Loss: 0.044, Data Loss: 0.001, Symnet Regularize: 0.868\n",
      "[PRINT] Epoch: 430, Loss: 0.042, Data Loss: 0.001, Symnet Regularize: 0.832\n",
      "[PRINT] Epoch: 440, Loss: 0.041, Data Loss: 0.001, Symnet Regularize: 0.796\n",
      "[PRINT] Epoch: 450, Loss: 0.039, Data Loss: 0.001, Symnet Regularize: 0.757\n",
      "[PRINT] Epoch: 460, Loss: 0.037, Data Loss: 0.001, Symnet Regularize: 0.718\n",
      "[PRINT] Epoch: 470, Loss: 0.035, Data Loss: 0.001, Symnet Regularize: 0.680\n",
      "[PRINT] Epoch: 480, Loss: 0.033, Data Loss: 0.001, Symnet Regularize: 0.642\n",
      "[PRINT] Epoch: 490, Loss: 0.031, Data Loss: 0.001, Symnet Regularize: 0.603\n",
      "[PRINT] block: 2\n",
      "[PRINT] Epoch: 0, Loss: 0.060, Data Loss: 0.002, Symnet Regularize: 0.576\n",
      "[PRINT] Epoch: 10, Loss: 0.058, Data Loss: 0.002, Symnet Regularize: 0.556\n",
      "[PRINT] Epoch: 20, Loss: 0.055, Data Loss: 0.002, Symnet Regularize: 0.529\n",
      "[PRINT] Epoch: 30, Loss: 0.052, Data Loss: 0.002, Symnet Regularize: 0.502\n",
      "[PRINT] Epoch: 40, Loss: 0.049, Data Loss: 0.002, Symnet Regularize: 0.473\n",
      "[PRINT] Epoch: 50, Loss: 0.046, Data Loss: 0.002, Symnet Regularize: 0.444\n",
      "[PRINT] Epoch: 60, Loss: 0.044, Data Loss: 0.002, Symnet Regularize: 0.415\n",
      "[PRINT] Epoch: 70, Loss: 0.041, Data Loss: 0.002, Symnet Regularize: 0.385\n",
      "[PRINT] Epoch: 80, Loss: 0.038, Data Loss: 0.002, Symnet Regularize: 0.356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 90, Loss: 0.035, Data Loss: 0.002, Symnet Regularize: 0.326\n",
      "[PRINT] Epoch: 100, Loss: 0.032, Data Loss: 0.002, Symnet Regularize: 0.295\n",
      "[PRINT] Epoch: 110, Loss: 0.029, Data Loss: 0.002, Symnet Regularize: 0.265\n",
      "[PRINT] Epoch: 120, Loss: 0.026, Data Loss: 0.002, Symnet Regularize: 0.234\n",
      "[PRINT] Epoch: 130, Loss: 0.022, Data Loss: 0.002, Symnet Regularize: 0.203\n",
      "[PRINT] Epoch: 140, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 0.172\n",
      "[PRINT] Epoch: 150, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.142\n",
      "[PRINT] Epoch: 160, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.125\n",
      "[PRINT] Epoch: 170, Loss: 0.013, Data Loss: 0.002, Symnet Regularize: 0.109\n",
      "[PRINT] Epoch: 180, Loss: 0.012, Data Loss: 0.002, Symnet Regularize: 0.094\n",
      "[PRINT] Epoch: 190, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.094\n",
      "[PRINT] Epoch: 200, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 210, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 220, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 230, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 240, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 250, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 260, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 270, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 280, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 290, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 300, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 310, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 320, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 330, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 340, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 350, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 360, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 370, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 380, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 390, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 400, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 410, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 420, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 430, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 440, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 450, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 460, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 470, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 480, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 490, Loss: 0.011, Data Loss: 0.002, Symnet Regularize: 0.093\n",
      "[PRINT] block: 3\n",
      "[PRINT] Epoch: 0, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 10, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 20, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 30, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 40, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 50, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 60, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 70, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 80, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 90, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 100, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 110, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 120, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 130, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 140, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 150, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 160, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 170, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 180, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 190, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 200, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 210, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 220, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 230, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 240, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 250, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 260, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 270, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 280, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 290, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 300, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 310, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 320, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 330, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 340, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 350, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 360, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 370, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 380, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 390, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 400, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 410, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 420, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 430, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 440, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 450, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 460, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 470, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 480, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 490, Loss: 0.018, Data Loss: 0.004, Symnet Regularize: 0.093\n",
      "[PRINT] block: 4\n",
      "[PRINT] Epoch: 0, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 10, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 20, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 30, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 40, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 50, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 60, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 70, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 80, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 90, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 100, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 110, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 120, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 130, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 140, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 150, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 160, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 170, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 180, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 190, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 200, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 210, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 220, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 230, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 240, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 250, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 260, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 270, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 280, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 290, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 300, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 310, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 320, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 330, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 340, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 350, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 360, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 370, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 380, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 390, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 400, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 410, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 420, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 430, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 440, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 450, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 460, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 470, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 480, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 490, Loss: 0.026, Data Loss: 0.008, Symnet Regularize: 0.093\n",
      "[PRINT] block: 5\n",
      "[PRINT] Epoch: 0, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 10, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 20, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 30, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 40, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 50, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 60, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 70, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 80, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 90, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 100, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 110, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 120, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 130, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 140, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 150, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 160, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 170, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 180, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 190, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 200, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 210, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 220, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 230, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 240, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 250, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 260, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 270, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 280, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 290, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 300, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 310, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 320, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 330, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 340, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 350, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 360, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 370, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 380, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 390, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 400, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 410, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 420, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 430, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 440, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 450, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 460, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 470, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 480, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 490, Loss: 0.036, Data Loss: 0.013, Symnet Regularize: 0.093\n",
      "[PRINT] block: 6\n",
      "[PRINT] Epoch: 0, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 10, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 20, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 30, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 40, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 50, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 60, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 70, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 80, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 90, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 100, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 110, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 120, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 130, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 140, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 150, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 160, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 170, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 180, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 190, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 200, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 210, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 220, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 230, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 240, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 250, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 260, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 270, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 280, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 290, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 300, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 310, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 320, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 330, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 340, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 350, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 360, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 370, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 380, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 390, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 400, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 410, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 420, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 430, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 440, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 450, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 460, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 470, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 480, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 490, Loss: 0.047, Data Loss: 0.019, Symnet Regularize: 0.092\n",
      "[PRINT] block: 7\n",
      "[PRINT] Epoch: 0, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 10, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 20, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 30, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 40, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 50, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 60, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 70, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 80, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 90, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 100, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 110, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 120, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 130, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 140, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 150, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 160, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 170, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 180, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 190, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 200, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 210, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 220, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 230, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 240, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 250, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 260, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 270, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 280, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 290, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 300, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 310, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 320, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 330, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 340, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 350, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 360, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 370, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 380, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 390, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 400, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 410, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 420, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 430, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 440, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 450, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 460, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 470, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 480, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 490, Loss: 0.060, Data Loss: 0.028, Symnet Regularize: 0.092\n",
      "[PRINT] block: 8\n",
      "[PRINT] Epoch: 0, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 10, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 20, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 30, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 40, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 50, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 60, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 70, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 80, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 90, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 100, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 110, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 120, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 130, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 140, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 150, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 160, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 170, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 180, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 190, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 200, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 210, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 220, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 230, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 240, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 250, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 260, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 270, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 280, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 290, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 300, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 310, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 320, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 330, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 340, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 350, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 360, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 370, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 380, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.093\n",
      "[PRINT] Epoch: 390, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 400, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 410, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 420, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 430, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 440, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 450, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 460, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 470, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n",
      "[PRINT] Epoch: 480, Loss: 0.075, Data Loss: 0.038, Symnet Regularize: 0.092\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-94ea17e35fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyment_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-2916054f5975>\u001b[0m in \u001b[0;36mmodelLoss\u001b[0;34m(model, u_obs, config, block)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmse_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mut_next_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#take one step from this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdata_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut_next_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut_next_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-63c43e3e41e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, step_num)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mstep_nums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdt\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcalculate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         '''\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultistep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-63c43e3e41e7>\u001b[0m in \u001b[0;36mmultistep\u001b[0;34m(self, inputs, step_num)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0muadd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRightHandItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#will take a dt step from u using the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0muadd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-63c43e3e41e7>\u001b[0m in \u001b[0;36mRightHandItems\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_diff_order\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mfd_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fd'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mu_deriv_order_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfd_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mu_derives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_deriv_order_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-198-06c1a0e404cd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0minput\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0mx\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         '''\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minp_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadInputTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#batch_size x n_channels x (x_dim+padded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#         print(inp_padded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ra/utils.py\u001b[0m in \u001b[0;36mpadInputTorch\u001b[0;34m(X, diff_order, acc_order, dim)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadStart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ra/utils.py\u001b[0m in \u001b[0;36mpadStart\u001b[0;34m(X, diff_order, acc_order)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_c\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mright_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mcentral_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_c\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentral_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mx_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mright_conv\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcentral_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcentral_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    u_obs = data_model.data(stepnum+1) #np array of stepnum elements\n",
    "    for epoch in range(epochs):\n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        loss,data_loss,syment_reg = modelLoss(model,u_obs,config,block)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f\" % (epoch,loss,\\\n",
    "                                                                                              data_loss,syment_reg))\n",
    "\n",
    "    \n",
    "# put prints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.00228215 u_{0} + 0.0968245 u_{2} + 0.00166485$"
      ],
      "text/plain": [
       "-0.00228215*u_0 + 0.0968245*u_2 + 0.00166485"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet.getEquation(calprec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('symnet.layer0.weight',\n",
       "              tensor([[ 2.2683e-04, -4.9045e-44, -2.8026e-45],\n",
       "                      [-2.8026e-45, -2.8026e-45,  2.8026e-45]])),\n",
       "             ('symnet.layer0.bias', tensor([ 4.2039e-44, -1.9478e-43])),\n",
       "             ('symnet.layer1.weight',\n",
       "              tensor([[ 8.0529e-05,  2.8026e-45,  5.6052e-45, -2.8026e-45],\n",
       "                      [-1.2234e-34, -9.1084e-44, -8.4078e-45,  1.4013e-45]])),\n",
       "             ('symnet.layer1.bias', tensor([ 2.7142e-08, -7.2868e-44])),\n",
       "             ('symnet.layer_final.weight',\n",
       "              tensor([[-2.2822e-03,  9.0344e-04,  9.6824e-02,  1.4013e-45, -5.6108e-42]])),\n",
       "             ('symnet.layer_final.bias', tensor([0.0017]))])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"Diffusion_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burger's Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_burgers.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Burgers Eqn',\n",
       " 'dt': 0.01,\n",
       " 'dx': 0.1,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 2,\n",
       " 'acc_order': 2,\n",
       " 'n_hidden_layers': 2,\n",
       " 'dataname': 'Diffusion',\n",
       " 'viscosity': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'channel_names': 'u',\n",
       " 'data_timescheme': 'rk4',\n",
       " 'data_dir': '/sdsd/dsds/sdsd',\n",
       " 'Nt': 100,\n",
       " 'Nx': 32,\n",
       " 'sigma': 1,\n",
       " 'sparsity': 0.05,\n",
       " 'epochs': 500,\n",
       " 'results_dir': '/comet/results/',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.005}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n",
      "[PRINT] Epoch: 0, Loss: 77.414, Data Loss: 77.414, Symnet Regularize: 6.930\n",
      "[PRINT] Epoch: 10, Loss: 21.137, Data Loss: 21.137, Symnet Regularize: 6.679\n",
      "[PRINT] Epoch: 20, Loss: 12.704, Data Loss: 12.704, Symnet Regularize: 6.479\n",
      "[PRINT] Epoch: 30, Loss: 9.997, Data Loss: 9.997, Symnet Regularize: 6.351\n",
      "[PRINT] Epoch: 40, Loss: 9.146, Data Loss: 9.146, Symnet Regularize: 6.274\n",
      "[PRINT] Epoch: 50, Loss: 8.112, Data Loss: 8.112, Symnet Regularize: 6.305\n",
      "[PRINT] Epoch: 60, Loss: 7.464, Data Loss: 7.464, Symnet Regularize: 6.350\n",
      "[PRINT] Epoch: 70, Loss: 6.999, Data Loss: 6.999, Symnet Regularize: 6.389\n",
      "[PRINT] Epoch: 80, Loss: 6.634, Data Loss: 6.634, Symnet Regularize: 6.429\n",
      "[PRINT] Epoch: 90, Loss: 6.337, Data Loss: 6.337, Symnet Regularize: 6.475\n",
      "[PRINT] Epoch: 100, Loss: 6.092, Data Loss: 6.092, Symnet Regularize: 6.521\n",
      "[PRINT] Epoch: 110, Loss: 5.877, Data Loss: 5.877, Symnet Regularize: 6.569\n",
      "[PRINT] Epoch: 120, Loss: 5.683, Data Loss: 5.683, Symnet Regularize: 6.618\n",
      "[PRINT] Epoch: 130, Loss: 5.501, Data Loss: 5.501, Symnet Regularize: 6.671\n",
      "[PRINT] Epoch: 140, Loss: 5.326, Data Loss: 5.326, Symnet Regularize: 6.727\n",
      "[PRINT] Epoch: 150, Loss: 5.150, Data Loss: 5.150, Symnet Regularize: 6.786\n",
      "[PRINT] Epoch: 160, Loss: 4.969, Data Loss: 4.969, Symnet Regularize: 6.848\n",
      "[PRINT] Epoch: 170, Loss: 4.778, Data Loss: 4.778, Symnet Regularize: 6.913\n",
      "[PRINT] Epoch: 180, Loss: 4.574, Data Loss: 4.574, Symnet Regularize: 6.981\n",
      "[PRINT] Epoch: 190, Loss: 4.353, Data Loss: 4.353, Symnet Regularize: 7.053\n",
      "[PRINT] Epoch: 200, Loss: 4.113, Data Loss: 4.113, Symnet Regularize: 7.129\n",
      "[PRINT] Epoch: 210, Loss: 3.852, Data Loss: 3.852, Symnet Regularize: 7.210\n",
      "[PRINT] Epoch: 220, Loss: 3.569, Data Loss: 3.569, Symnet Regularize: 7.296\n",
      "[PRINT] Epoch: 230, Loss: 3.267, Data Loss: 3.267, Symnet Regularize: 7.388\n",
      "[PRINT] Epoch: 240, Loss: 2.950, Data Loss: 2.950, Symnet Regularize: 7.486\n",
      "[PRINT] Epoch: 250, Loss: 2.624, Data Loss: 2.624, Symnet Regularize: 7.587\n",
      "[PRINT] Epoch: 260, Loss: 2.298, Data Loss: 2.298, Symnet Regularize: 7.688\n",
      "[PRINT] Epoch: 270, Loss: 1.981, Data Loss: 1.981, Symnet Regularize: 7.785\n",
      "[PRINT] Epoch: 280, Loss: 1.683, Data Loss: 1.683, Symnet Regularize: 7.874\n",
      "[PRINT] Epoch: 290, Loss: 1.409, Data Loss: 1.409, Symnet Regularize: 7.950\n",
      "[PRINT] Epoch: 300, Loss: 1.165, Data Loss: 1.165, Symnet Regularize: 8.011\n",
      "[PRINT] Epoch: 310, Loss: 0.952, Data Loss: 0.952, Symnet Regularize: 8.054\n",
      "[PRINT] Epoch: 320, Loss: 0.771, Data Loss: 0.771, Symnet Regularize: 8.081\n",
      "[PRINT] Epoch: 330, Loss: 0.619, Data Loss: 0.619, Symnet Regularize: 8.100\n",
      "[PRINT] Epoch: 340, Loss: 0.496, Data Loss: 0.496, Symnet Regularize: 8.133\n",
      "[PRINT] Epoch: 350, Loss: 0.398, Data Loss: 0.398, Symnet Regularize: 8.158\n",
      "[PRINT] Epoch: 360, Loss: 0.322, Data Loss: 0.322, Symnet Regularize: 8.172\n",
      "[PRINT] Epoch: 370, Loss: 0.264, Data Loss: 0.264, Symnet Regularize: 8.179\n",
      "[PRINT] Epoch: 380, Loss: 0.221, Data Loss: 0.221, Symnet Regularize: 8.179\n",
      "[PRINT] Epoch: 390, Loss: 0.189, Data Loss: 0.189, Symnet Regularize: 8.175\n",
      "[PRINT] Epoch: 400, Loss: 0.165, Data Loss: 0.165, Symnet Regularize: 8.171\n",
      "[PRINT] Epoch: 410, Loss: 0.148, Data Loss: 0.148, Symnet Regularize: 8.172\n",
      "[PRINT] Epoch: 420, Loss: 0.135, Data Loss: 0.135, Symnet Regularize: 8.177\n",
      "[PRINT] Epoch: 430, Loss: 0.125, Data Loss: 0.125, Symnet Regularize: 8.180\n",
      "[PRINT] Epoch: 440, Loss: 0.117, Data Loss: 0.117, Symnet Regularize: 8.182\n",
      "[PRINT] Epoch: 450, Loss: 0.111, Data Loss: 0.111, Symnet Regularize: 8.182\n",
      "[PRINT] Epoch: 460, Loss: 0.105, Data Loss: 0.105, Symnet Regularize: 8.181\n",
      "[PRINT] Epoch: 470, Loss: 0.101, Data Loss: 0.101, Symnet Regularize: 8.178\n",
      "[PRINT] Epoch: 480, Loss: 0.097, Data Loss: 0.097, Symnet Regularize: 8.174\n",
      "[PRINT] Epoch: 490, Loss: 0.093, Data Loss: 0.093, Symnet Regularize: 8.170\n",
      "[PRINT] Epoch: 500, Loss: 0.089, Data Loss: 0.089, Symnet Regularize: 8.165\n",
      "[PRINT] Epoch: 510, Loss: 0.086, Data Loss: 0.086, Symnet Regularize: 8.160\n",
      "[PRINT] Epoch: 520, Loss: 0.083, Data Loss: 0.083, Symnet Regularize: 8.154\n",
      "[PRINT] Epoch: 530, Loss: 0.080, Data Loss: 0.080, Symnet Regularize: 8.149\n",
      "[PRINT] Epoch: 540, Loss: 0.077, Data Loss: 0.077, Symnet Regularize: 8.143\n",
      "[PRINT] Epoch: 550, Loss: 0.074, Data Loss: 0.074, Symnet Regularize: 8.137\n",
      "[PRINT] Epoch: 560, Loss: 0.071, Data Loss: 0.071, Symnet Regularize: 8.131\n",
      "[PRINT] Epoch: 570, Loss: 0.068, Data Loss: 0.068, Symnet Regularize: 8.125\n",
      "[PRINT] Epoch: 580, Loss: 0.066, Data Loss: 0.066, Symnet Regularize: 8.120\n",
      "[PRINT] Epoch: 590, Loss: 0.063, Data Loss: 0.063, Symnet Regularize: 8.114\n",
      "[PRINT] Epoch: 600, Loss: 0.061, Data Loss: 0.061, Symnet Regularize: 8.108\n",
      "[PRINT] Epoch: 610, Loss: 0.058, Data Loss: 0.058, Symnet Regularize: 8.103\n",
      "[PRINT] Epoch: 620, Loss: 0.056, Data Loss: 0.056, Symnet Regularize: 8.098\n",
      "[PRINT] Epoch: 630, Loss: 0.054, Data Loss: 0.054, Symnet Regularize: 8.092\n",
      "[PRINT] Epoch: 640, Loss: 0.052, Data Loss: 0.052, Symnet Regularize: 8.087\n",
      "[PRINT] Epoch: 650, Loss: 0.050, Data Loss: 0.050, Symnet Regularize: 8.082\n",
      "[PRINT] Epoch: 660, Loss: 0.048, Data Loss: 0.048, Symnet Regularize: 8.077\n",
      "[PRINT] Epoch: 670, Loss: 0.046, Data Loss: 0.046, Symnet Regularize: 8.073\n",
      "[PRINT] Epoch: 680, Loss: 0.044, Data Loss: 0.044, Symnet Regularize: 8.068\n",
      "[PRINT] Epoch: 690, Loss: 0.043, Data Loss: 0.043, Symnet Regularize: 8.064\n",
      "[PRINT] Epoch: 700, Loss: 0.041, Data Loss: 0.041, Symnet Regularize: 8.059\n",
      "[PRINT] Epoch: 710, Loss: 0.039, Data Loss: 0.039, Symnet Regularize: 8.055\n",
      "[PRINT] Epoch: 720, Loss: 0.038, Data Loss: 0.038, Symnet Regularize: 8.051\n",
      "[PRINT] Epoch: 730, Loss: 0.036, Data Loss: 0.036, Symnet Regularize: 8.047\n",
      "[PRINT] Epoch: 740, Loss: 0.035, Data Loss: 0.035, Symnet Regularize: 8.043\n",
      "[PRINT] Epoch: 750, Loss: 0.034, Data Loss: 0.034, Symnet Regularize: 8.039\n",
      "[PRINT] Epoch: 760, Loss: 0.032, Data Loss: 0.032, Symnet Regularize: 8.036\n",
      "[PRINT] Epoch: 770, Loss: 0.031, Data Loss: 0.031, Symnet Regularize: 8.032\n",
      "[PRINT] Epoch: 780, Loss: 0.030, Data Loss: 0.030, Symnet Regularize: 8.029\n",
      "[PRINT] Epoch: 790, Loss: 0.029, Data Loss: 0.029, Symnet Regularize: 8.026\n",
      "[PRINT] Epoch: 800, Loss: 0.028, Data Loss: 0.028, Symnet Regularize: 8.023\n",
      "[PRINT] Epoch: 810, Loss: 0.027, Data Loss: 0.027, Symnet Regularize: 8.020\n",
      "[PRINT] Epoch: 820, Loss: 0.026, Data Loss: 0.026, Symnet Regularize: 8.017\n",
      "[PRINT] Epoch: 830, Loss: 0.025, Data Loss: 0.025, Symnet Regularize: 8.014\n",
      "[PRINT] Epoch: 840, Loss: 0.024, Data Loss: 0.024, Symnet Regularize: 8.011\n",
      "[PRINT] Epoch: 850, Loss: 0.023, Data Loss: 0.023, Symnet Regularize: 8.008\n",
      "[PRINT] Epoch: 860, Loss: 0.022, Data Loss: 0.022, Symnet Regularize: 8.006\n",
      "[PRINT] Epoch: 870, Loss: 0.022, Data Loss: 0.022, Symnet Regularize: 8.003\n",
      "[PRINT] Epoch: 880, Loss: 0.021, Data Loss: 0.021, Symnet Regularize: 8.001\n",
      "[PRINT] Epoch: 890, Loss: 0.020, Data Loss: 0.020, Symnet Regularize: 7.999\n",
      "[PRINT] Epoch: 900, Loss: 0.019, Data Loss: 0.019, Symnet Regularize: 7.997\n",
      "[PRINT] Epoch: 910, Loss: 0.019, Data Loss: 0.019, Symnet Regularize: 7.994\n",
      "[PRINT] Epoch: 920, Loss: 0.018, Data Loss: 0.018, Symnet Regularize: 7.992\n",
      "[PRINT] Epoch: 930, Loss: 0.018, Data Loss: 0.018, Symnet Regularize: 7.990\n",
      "[PRINT] Epoch: 940, Loss: 0.017, Data Loss: 0.017, Symnet Regularize: 7.988\n",
      "[PRINT] Epoch: 950, Loss: 0.017, Data Loss: 0.017, Symnet Regularize: 7.987\n",
      "[PRINT] Epoch: 960, Loss: 0.016, Data Loss: 0.016, Symnet Regularize: 7.985\n",
      "[PRINT] Epoch: 970, Loss: 0.016, Data Loss: 0.016, Symnet Regularize: 7.983\n",
      "[PRINT] Epoch: 980, Loss: 0.015, Data Loss: 0.015, Symnet Regularize: 7.981\n",
      "[PRINT] Epoch: 990, Loss: 0.015, Data Loss: 0.015, Symnet Regularize: 7.980\n",
      "[PRINT] block: 1\n",
      "[PRINT] Epoch: 0, Loss: 0.413, Data Loss: 0.014, Symnet Regularize: 7.978\n",
      "[PRINT] Epoch: 10, Loss: 0.412, Data Loss: 0.014, Symnet Regularize: 7.950\n",
      "[PRINT] Epoch: 20, Loss: 0.409, Data Loss: 0.014, Symnet Regularize: 7.906\n",
      "[PRINT] Epoch: 30, Loss: 0.407, Data Loss: 0.014, Symnet Regularize: 7.859\n",
      "[PRINT] Epoch: 40, Loss: 0.404, Data Loss: 0.013, Symnet Regularize: 7.812\n",
      "[PRINT] Epoch: 50, Loss: 0.401, Data Loss: 0.013, Symnet Regularize: 7.765\n",
      "[PRINT] Epoch: 60, Loss: 0.399, Data Loss: 0.013, Symnet Regularize: 7.717\n",
      "[PRINT] Epoch: 70, Loss: 0.396, Data Loss: 0.012, Symnet Regularize: 7.668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 80, Loss: 0.393, Data Loss: 0.012, Symnet Regularize: 7.620\n",
      "[PRINT] Epoch: 90, Loss: 0.390, Data Loss: 0.012, Symnet Regularize: 7.571\n",
      "[PRINT] Epoch: 100, Loss: 0.388, Data Loss: 0.012, Symnet Regularize: 7.521\n",
      "[PRINT] Epoch: 110, Loss: 0.385, Data Loss: 0.011, Symnet Regularize: 7.472\n",
      "[PRINT] Epoch: 120, Loss: 0.382, Data Loss: 0.011, Symnet Regularize: 7.422\n",
      "[PRINT] Epoch: 130, Loss: 0.380, Data Loss: 0.011, Symnet Regularize: 7.372\n",
      "[PRINT] Epoch: 140, Loss: 0.377, Data Loss: 0.011, Symnet Regularize: 7.322\n",
      "[PRINT] Epoch: 150, Loss: 0.374, Data Loss: 0.011, Symnet Regularize: 7.271\n",
      "[PRINT] Epoch: 160, Loss: 0.371, Data Loss: 0.010, Symnet Regularize: 7.220\n",
      "[PRINT] Epoch: 170, Loss: 0.369, Data Loss: 0.010, Symnet Regularize: 7.169\n",
      "[PRINT] Epoch: 180, Loss: 0.366, Data Loss: 0.010, Symnet Regularize: 7.119\n",
      "[PRINT] Epoch: 190, Loss: 0.364, Data Loss: 0.010, Symnet Regularize: 7.072\n",
      "[PRINT] Epoch: 200, Loss: 0.361, Data Loss: 0.010, Symnet Regularize: 7.027\n",
      "[PRINT] Epoch: 210, Loss: 0.359, Data Loss: 0.010, Symnet Regularize: 6.983\n",
      "[PRINT] Epoch: 220, Loss: 0.357, Data Loss: 0.010, Symnet Regularize: 6.941\n",
      "[PRINT] Epoch: 230, Loss: 0.355, Data Loss: 0.010, Symnet Regularize: 6.899\n",
      "[PRINT] Epoch: 240, Loss: 0.353, Data Loss: 0.010, Symnet Regularize: 6.858\n",
      "[PRINT] Epoch: 250, Loss: 0.351, Data Loss: 0.010, Symnet Regularize: 6.818\n",
      "[PRINT] Epoch: 260, Loss: 0.349, Data Loss: 0.010, Symnet Regularize: 6.778\n",
      "[PRINT] Epoch: 270, Loss: 0.347, Data Loss: 0.010, Symnet Regularize: 6.738\n",
      "[PRINT] Epoch: 280, Loss: 0.345, Data Loss: 0.010, Symnet Regularize: 6.700\n",
      "[PRINT] Epoch: 290, Loss: 0.343, Data Loss: 0.010, Symnet Regularize: 6.663\n",
      "[PRINT] Epoch: 300, Loss: 0.341, Data Loss: 0.010, Symnet Regularize: 6.626\n",
      "[PRINT] Epoch: 310, Loss: 0.339, Data Loss: 0.010, Symnet Regularize: 6.590\n",
      "[PRINT] Epoch: 320, Loss: 0.337, Data Loss: 0.010, Symnet Regularize: 6.554\n",
      "[PRINT] Epoch: 330, Loss: 0.336, Data Loss: 0.010, Symnet Regularize: 6.518\n",
      "[PRINT] Epoch: 340, Loss: 0.334, Data Loss: 0.010, Symnet Regularize: 6.482\n",
      "[PRINT] Epoch: 350, Loss: 0.332, Data Loss: 0.010, Symnet Regularize: 6.446\n",
      "[PRINT] Epoch: 360, Loss: 0.330, Data Loss: 0.010, Symnet Regularize: 6.409\n",
      "[PRINT] Epoch: 370, Loss: 0.328, Data Loss: 0.010, Symnet Regularize: 6.372\n",
      "[PRINT] Epoch: 380, Loss: 0.326, Data Loss: 0.010, Symnet Regularize: 6.336\n",
      "[PRINT] Epoch: 390, Loss: 0.325, Data Loss: 0.010, Symnet Regularize: 6.299\n",
      "[PRINT] Epoch: 400, Loss: 0.323, Data Loss: 0.010, Symnet Regularize: 6.262\n",
      "[PRINT] Epoch: 410, Loss: 0.321, Data Loss: 0.010, Symnet Regularize: 6.225\n",
      "[PRINT] Epoch: 420, Loss: 0.319, Data Loss: 0.010, Symnet Regularize: 6.188\n",
      "[PRINT] Epoch: 430, Loss: 0.317, Data Loss: 0.010, Symnet Regularize: 6.150\n",
      "[PRINT] Epoch: 440, Loss: 0.315, Data Loss: 0.009, Symnet Regularize: 6.113\n",
      "[PRINT] Epoch: 450, Loss: 0.313, Data Loss: 0.009, Symnet Regularize: 6.076\n",
      "[PRINT] Epoch: 460, Loss: 0.311, Data Loss: 0.009, Symnet Regularize: 6.039\n",
      "[PRINT] Epoch: 470, Loss: 0.310, Data Loss: 0.009, Symnet Regularize: 6.002\n",
      "[PRINT] Epoch: 480, Loss: 0.308, Data Loss: 0.009, Symnet Regularize: 5.965\n",
      "[PRINT] Epoch: 490, Loss: 0.306, Data Loss: 0.009, Symnet Regularize: 5.928\n",
      "[PRINT] Epoch: 500, Loss: 0.304, Data Loss: 0.009, Symnet Regularize: 5.891\n",
      "[PRINT] Epoch: 510, Loss: 0.302, Data Loss: 0.009, Symnet Regularize: 5.855\n",
      "[PRINT] Epoch: 520, Loss: 0.300, Data Loss: 0.009, Symnet Regularize: 5.823\n",
      "[PRINT] Epoch: 530, Loss: 0.299, Data Loss: 0.009, Symnet Regularize: 5.791\n",
      "[PRINT] Epoch: 540, Loss: 0.297, Data Loss: 0.009, Symnet Regularize: 5.760\n",
      "[PRINT] Epoch: 550, Loss: 0.296, Data Loss: 0.009, Symnet Regularize: 5.730\n",
      "[PRINT] Epoch: 560, Loss: 0.294, Data Loss: 0.009, Symnet Regularize: 5.702\n",
      "[PRINT] Epoch: 570, Loss: 0.293, Data Loss: 0.009, Symnet Regularize: 5.677\n",
      "[PRINT] Epoch: 580, Loss: 0.291, Data Loss: 0.009, Symnet Regularize: 5.652\n",
      "[PRINT] Epoch: 590, Loss: 0.290, Data Loss: 0.009, Symnet Regularize: 5.627\n",
      "[PRINT] Epoch: 600, Loss: 0.289, Data Loss: 0.009, Symnet Regularize: 5.603\n",
      "[PRINT] Epoch: 610, Loss: 0.288, Data Loss: 0.009, Symnet Regularize: 5.579\n",
      "[PRINT] Epoch: 620, Loss: 0.287, Data Loss: 0.009, Symnet Regularize: 5.555\n",
      "[PRINT] Epoch: 630, Loss: 0.285, Data Loss: 0.009, Symnet Regularize: 5.531\n",
      "[PRINT] Epoch: 640, Loss: 0.284, Data Loss: 0.009, Symnet Regularize: 5.508\n",
      "[PRINT] Epoch: 650, Loss: 0.283, Data Loss: 0.009, Symnet Regularize: 5.487\n",
      "[PRINT] Epoch: 660, Loss: 0.282, Data Loss: 0.009, Symnet Regularize: 5.467\n",
      "[PRINT] Epoch: 670, Loss: 0.281, Data Loss: 0.009, Symnet Regularize: 5.447\n",
      "[PRINT] Epoch: 680, Loss: 0.280, Data Loss: 0.009, Symnet Regularize: 5.428\n",
      "[PRINT] Epoch: 690, Loss: 0.279, Data Loss: 0.009, Symnet Regularize: 5.409\n",
      "[PRINT] Epoch: 700, Loss: 0.278, Data Loss: 0.009, Symnet Regularize: 5.391\n",
      "[PRINT] Epoch: 710, Loss: 0.278, Data Loss: 0.009, Symnet Regularize: 5.376\n",
      "[PRINT] Epoch: 720, Loss: 0.277, Data Loss: 0.009, Symnet Regularize: 5.369\n",
      "[PRINT] Epoch: 730, Loss: 0.277, Data Loss: 0.009, Symnet Regularize: 5.360\n",
      "[PRINT] Epoch: 740, Loss: 0.276, Data Loss: 0.009, Symnet Regularize: 5.351\n",
      "[PRINT] Epoch: 750, Loss: 0.276, Data Loss: 0.009, Symnet Regularize: 5.342\n",
      "[PRINT] Epoch: 760, Loss: 0.275, Data Loss: 0.009, Symnet Regularize: 5.334\n",
      "[PRINT] Epoch: 770, Loss: 0.275, Data Loss: 0.009, Symnet Regularize: 5.325\n",
      "[PRINT] Epoch: 780, Loss: 0.275, Data Loss: 0.009, Symnet Regularize: 5.317\n",
      "[PRINT] Epoch: 790, Loss: 0.274, Data Loss: 0.009, Symnet Regularize: 5.308\n",
      "[PRINT] Epoch: 800, Loss: 0.274, Data Loss: 0.009, Symnet Regularize: 5.299\n",
      "[PRINT] Epoch: 810, Loss: 0.273, Data Loss: 0.009, Symnet Regularize: 5.290\n",
      "[PRINT] Epoch: 820, Loss: 0.273, Data Loss: 0.009, Symnet Regularize: 5.282\n",
      "[PRINT] Epoch: 830, Loss: 0.272, Data Loss: 0.009, Symnet Regularize: 5.273\n",
      "[PRINT] Epoch: 840, Loss: 0.272, Data Loss: 0.009, Symnet Regularize: 5.264\n",
      "[PRINT] Epoch: 850, Loss: 0.272, Data Loss: 0.009, Symnet Regularize: 5.255\n",
      "[PRINT] Epoch: 860, Loss: 0.271, Data Loss: 0.009, Symnet Regularize: 5.246\n",
      "[PRINT] Epoch: 870, Loss: 0.271, Data Loss: 0.009, Symnet Regularize: 5.237\n",
      "[PRINT] Epoch: 880, Loss: 0.270, Data Loss: 0.009, Symnet Regularize: 5.228\n",
      "[PRINT] Epoch: 890, Loss: 0.270, Data Loss: 0.009, Symnet Regularize: 5.219\n",
      "[PRINT] Epoch: 900, Loss: 0.269, Data Loss: 0.009, Symnet Regularize: 5.210\n",
      "[PRINT] Epoch: 910, Loss: 0.269, Data Loss: 0.009, Symnet Regularize: 5.201\n",
      "[PRINT] Epoch: 920, Loss: 0.268, Data Loss: 0.009, Symnet Regularize: 5.192\n",
      "[PRINT] Epoch: 930, Loss: 0.268, Data Loss: 0.009, Symnet Regularize: 5.183\n",
      "[PRINT] Epoch: 940, Loss: 0.268, Data Loss: 0.009, Symnet Regularize: 5.174\n",
      "[PRINT] Epoch: 950, Loss: 0.267, Data Loss: 0.009, Symnet Regularize: 5.164\n",
      "[PRINT] Epoch: 960, Loss: 0.267, Data Loss: 0.009, Symnet Regularize: 5.155\n",
      "[PRINT] Epoch: 970, Loss: 0.266, Data Loss: 0.009, Symnet Regularize: 5.146\n",
      "[PRINT] Epoch: 980, Loss: 0.266, Data Loss: 0.009, Symnet Regularize: 5.136\n",
      "[PRINT] Epoch: 990, Loss: 0.265, Data Loss: 0.009, Symnet Regularize: 5.127\n",
      "[PRINT] block: 2\n",
      "[PRINT] Epoch: 0, Loss: 0.535, Data Loss: 0.023, Symnet Regularize: 5.118\n",
      "[PRINT] Epoch: 10, Loss: 0.533, Data Loss: 0.022, Symnet Regularize: 5.107\n",
      "[PRINT] Epoch: 20, Loss: 0.532, Data Loss: 0.022, Symnet Regularize: 5.090\n",
      "[PRINT] Epoch: 30, Loss: 0.530, Data Loss: 0.023, Symnet Regularize: 5.072\n",
      "[PRINT] Epoch: 40, Loss: 0.528, Data Loss: 0.023, Symnet Regularize: 5.053\n",
      "[PRINT] Epoch: 50, Loss: 0.526, Data Loss: 0.023, Symnet Regularize: 5.034\n",
      "[PRINT] Epoch: 60, Loss: 0.524, Data Loss: 0.023, Symnet Regularize: 5.015\n",
      "[PRINT] Epoch: 70, Loss: 0.522, Data Loss: 0.023, Symnet Regularize: 4.996\n",
      "[PRINT] Epoch: 80, Loss: 0.520, Data Loss: 0.023, Symnet Regularize: 4.977\n",
      "[PRINT] Epoch: 90, Loss: 0.519, Data Loss: 0.023, Symnet Regularize: 4.957\n",
      "[PRINT] Epoch: 100, Loss: 0.517, Data Loss: 0.023, Symnet Regularize: 4.938\n",
      "[PRINT] Epoch: 110, Loss: 0.515, Data Loss: 0.023, Symnet Regularize: 4.919\n",
      "[PRINT] Epoch: 120, Loss: 0.513, Data Loss: 0.023, Symnet Regularize: 4.899\n",
      "[PRINT] Epoch: 130, Loss: 0.511, Data Loss: 0.023, Symnet Regularize: 4.880\n",
      "[PRINT] Epoch: 140, Loss: 0.509, Data Loss: 0.023, Symnet Regularize: 4.860\n",
      "[PRINT] Epoch: 150, Loss: 0.507, Data Loss: 0.023, Symnet Regularize: 4.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 160, Loss: 0.505, Data Loss: 0.023, Symnet Regularize: 4.821\n",
      "[PRINT] Epoch: 170, Loss: 0.504, Data Loss: 0.023, Symnet Regularize: 4.802\n",
      "[PRINT] Epoch: 180, Loss: 0.502, Data Loss: 0.023, Symnet Regularize: 4.782\n",
      "[PRINT] Epoch: 190, Loss: 0.500, Data Loss: 0.023, Symnet Regularize: 4.763\n",
      "[PRINT] Epoch: 200, Loss: 0.498, Data Loss: 0.023, Symnet Regularize: 4.744\n",
      "[PRINT] Epoch: 210, Loss: 0.496, Data Loss: 0.024, Symnet Regularize: 4.726\n",
      "[PRINT] Epoch: 220, Loss: 0.494, Data Loss: 0.024, Symnet Regularize: 4.708\n",
      "[PRINT] Epoch: 230, Loss: 0.493, Data Loss: 0.024, Symnet Regularize: 4.690\n",
      "[PRINT] Epoch: 240, Loss: 0.491, Data Loss: 0.024, Symnet Regularize: 4.676\n",
      "[PRINT] Epoch: 250, Loss: 0.490, Data Loss: 0.024, Symnet Regularize: 4.666\n",
      "[PRINT] Epoch: 260, Loss: 0.490, Data Loss: 0.024, Symnet Regularize: 4.656\n",
      "[PRINT] Epoch: 270, Loss: 0.489, Data Loss: 0.024, Symnet Regularize: 4.646\n",
      "[PRINT] Epoch: 280, Loss: 0.488, Data Loss: 0.024, Symnet Regularize: 4.636\n",
      "[PRINT] Epoch: 290, Loss: 0.487, Data Loss: 0.024, Symnet Regularize: 4.626\n",
      "[PRINT] Epoch: 300, Loss: 0.486, Data Loss: 0.024, Symnet Regularize: 4.615\n",
      "[PRINT] Epoch: 310, Loss: 0.485, Data Loss: 0.024, Symnet Regularize: 4.605\n",
      "[PRINT] Epoch: 320, Loss: 0.484, Data Loss: 0.024, Symnet Regularize: 4.595\n",
      "[PRINT] Epoch: 330, Loss: 0.483, Data Loss: 0.025, Symnet Regularize: 4.584\n",
      "[PRINT] Epoch: 340, Loss: 0.482, Data Loss: 0.025, Symnet Regularize: 4.573\n",
      "[PRINT] Epoch: 350, Loss: 0.481, Data Loss: 0.025, Symnet Regularize: 4.563\n",
      "[PRINT] Epoch: 360, Loss: 0.480, Data Loss: 0.025, Symnet Regularize: 4.552\n",
      "[PRINT] Epoch: 370, Loss: 0.479, Data Loss: 0.025, Symnet Regularize: 4.541\n",
      "[PRINT] Epoch: 380, Loss: 0.478, Data Loss: 0.025, Symnet Regularize: 4.530\n",
      "[PRINT] Epoch: 390, Loss: 0.477, Data Loss: 0.025, Symnet Regularize: 4.519\n",
      "[PRINT] Epoch: 400, Loss: 0.476, Data Loss: 0.025, Symnet Regularize: 4.508\n",
      "[PRINT] Epoch: 410, Loss: 0.475, Data Loss: 0.025, Symnet Regularize: 4.497\n",
      "[PRINT] Epoch: 420, Loss: 0.474, Data Loss: 0.026, Symnet Regularize: 4.485\n",
      "[PRINT] Epoch: 430, Loss: 0.473, Data Loss: 0.026, Symnet Regularize: 4.474\n",
      "[PRINT] Epoch: 440, Loss: 0.472, Data Loss: 0.026, Symnet Regularize: 4.462\n",
      "[PRINT] Epoch: 450, Loss: 0.471, Data Loss: 0.026, Symnet Regularize: 4.451\n",
      "[PRINT] Epoch: 460, Loss: 0.470, Data Loss: 0.026, Symnet Regularize: 4.439\n",
      "[PRINT] Epoch: 470, Loss: 0.469, Data Loss: 0.026, Symnet Regularize: 4.427\n",
      "[PRINT] Epoch: 480, Loss: 0.468, Data Loss: 0.026, Symnet Regularize: 4.416\n",
      "[PRINT] Epoch: 490, Loss: 0.467, Data Loss: 0.027, Symnet Regularize: 4.404\n",
      "[PRINT] Epoch: 500, Loss: 0.466, Data Loss: 0.027, Symnet Regularize: 4.391\n",
      "[PRINT] Epoch: 510, Loss: 0.465, Data Loss: 0.027, Symnet Regularize: 4.379\n",
      "[PRINT] Epoch: 520, Loss: 0.464, Data Loss: 0.027, Symnet Regularize: 4.367\n",
      "[PRINT] Epoch: 530, Loss: 0.463, Data Loss: 0.027, Symnet Regularize: 4.355\n",
      "[PRINT] Epoch: 540, Loss: 0.461, Data Loss: 0.027, Symnet Regularize: 4.342\n",
      "[PRINT] Epoch: 550, Loss: 0.460, Data Loss: 0.027, Symnet Regularize: 4.330\n",
      "[PRINT] Epoch: 560, Loss: 0.459, Data Loss: 0.028, Symnet Regularize: 4.317\n",
      "[PRINT] Epoch: 570, Loss: 0.458, Data Loss: 0.028, Symnet Regularize: 4.304\n",
      "[PRINT] Epoch: 580, Loss: 0.457, Data Loss: 0.028, Symnet Regularize: 4.291\n",
      "[PRINT] Epoch: 590, Loss: 0.456, Data Loss: 0.028, Symnet Regularize: 4.278\n",
      "[PRINT] Epoch: 600, Loss: 0.455, Data Loss: 0.028, Symnet Regularize: 4.265\n",
      "[PRINT] Epoch: 610, Loss: 0.454, Data Loss: 0.028, Symnet Regularize: 4.251\n",
      "[PRINT] Epoch: 620, Loss: 0.452, Data Loss: 0.029, Symnet Regularize: 4.238\n",
      "[PRINT] Epoch: 630, Loss: 0.451, Data Loss: 0.029, Symnet Regularize: 4.224\n",
      "[PRINT] Epoch: 640, Loss: 0.450, Data Loss: 0.029, Symnet Regularize: 4.210\n",
      "[PRINT] Epoch: 650, Loss: 0.449, Data Loss: 0.029, Symnet Regularize: 4.196\n",
      "[PRINT] Epoch: 660, Loss: 0.448, Data Loss: 0.029, Symnet Regularize: 4.182\n",
      "[PRINT] Epoch: 670, Loss: 0.446, Data Loss: 0.030, Symnet Regularize: 4.168\n",
      "[PRINT] Epoch: 680, Loss: 0.445, Data Loss: 0.030, Symnet Regularize: 4.153\n",
      "[PRINT] Epoch: 690, Loss: 0.444, Data Loss: 0.030, Symnet Regularize: 4.139\n",
      "[PRINT] Epoch: 700, Loss: 0.442, Data Loss: 0.030, Symnet Regularize: 4.124\n",
      "[PRINT] Epoch: 710, Loss: 0.441, Data Loss: 0.030, Symnet Regularize: 4.109\n",
      "[PRINT] Epoch: 720, Loss: 0.440, Data Loss: 0.030, Symnet Regularize: 4.093\n",
      "[PRINT] Epoch: 730, Loss: 0.438, Data Loss: 0.031, Symnet Regularize: 4.078\n",
      "[PRINT] Epoch: 740, Loss: 0.437, Data Loss: 0.031, Symnet Regularize: 4.062\n",
      "[PRINT] Epoch: 750, Loss: 0.436, Data Loss: 0.031, Symnet Regularize: 4.046\n",
      "[PRINT] Epoch: 760, Loss: 0.434, Data Loss: 0.031, Symnet Regularize: 4.030\n",
      "[PRINT] Epoch: 770, Loss: 0.433, Data Loss: 0.031, Symnet Regularize: 4.014\n",
      "[PRINT] Epoch: 780, Loss: 0.431, Data Loss: 0.032, Symnet Regularize: 3.997\n",
      "[PRINT] Epoch: 790, Loss: 0.430, Data Loss: 0.032, Symnet Regularize: 3.980\n",
      "[PRINT] Epoch: 800, Loss: 0.428, Data Loss: 0.032, Symnet Regularize: 3.963\n",
      "[PRINT] Epoch: 810, Loss: 0.427, Data Loss: 0.032, Symnet Regularize: 3.946\n",
      "[PRINT] Epoch: 820, Loss: 0.425, Data Loss: 0.032, Symnet Regularize: 3.928\n",
      "[PRINT] Epoch: 830, Loss: 0.423, Data Loss: 0.032, Symnet Regularize: 3.911\n",
      "[PRINT] Epoch: 840, Loss: 0.422, Data Loss: 0.032, Symnet Regularize: 3.893\n",
      "[PRINT] Epoch: 850, Loss: 0.420, Data Loss: 0.033, Symnet Regularize: 3.878\n",
      "[PRINT] Epoch: 860, Loss: 0.420, Data Loss: 0.033, Symnet Regularize: 3.870\n",
      "[PRINT] Epoch: 870, Loss: 0.419, Data Loss: 0.033, Symnet Regularize: 3.862\n",
      "[PRINT] Epoch: 880, Loss: 0.418, Data Loss: 0.033, Symnet Regularize: 3.854\n",
      "[PRINT] Epoch: 890, Loss: 0.417, Data Loss: 0.033, Symnet Regularize: 3.847\n",
      "[PRINT] Epoch: 900, Loss: 0.417, Data Loss: 0.033, Symnet Regularize: 3.839\n",
      "[PRINT] Epoch: 910, Loss: 0.416, Data Loss: 0.033, Symnet Regularize: 3.832\n",
      "[PRINT] Epoch: 920, Loss: 0.415, Data Loss: 0.033, Symnet Regularize: 3.825\n",
      "[PRINT] Epoch: 930, Loss: 0.415, Data Loss: 0.033, Symnet Regularize: 3.817\n",
      "[PRINT] Epoch: 940, Loss: 0.414, Data Loss: 0.033, Symnet Regularize: 3.810\n",
      "[PRINT] Epoch: 950, Loss: 0.414, Data Loss: 0.033, Symnet Regularize: 3.803\n",
      "[PRINT] Epoch: 960, Loss: 0.413, Data Loss: 0.033, Symnet Regularize: 3.796\n",
      "[PRINT] Epoch: 970, Loss: 0.412, Data Loss: 0.034, Symnet Regularize: 3.789\n",
      "[PRINT] Epoch: 980, Loss: 0.412, Data Loss: 0.034, Symnet Regularize: 3.783\n",
      "[PRINT] Epoch: 990, Loss: 0.411, Data Loss: 0.034, Symnet Regularize: 3.776\n",
      "[PRINT] block: 3\n",
      "[PRINT] Epoch: 0, Loss: 0.629, Data Loss: 0.064, Symnet Regularize: 3.770\n",
      "[PRINT] Epoch: 10, Loss: 0.628, Data Loss: 0.063, Symnet Regularize: 3.765\n",
      "[PRINT] Epoch: 20, Loss: 0.627, Data Loss: 0.063, Symnet Regularize: 3.758\n",
      "[PRINT] Epoch: 30, Loss: 0.626, Data Loss: 0.064, Symnet Regularize: 3.750\n",
      "[PRINT] Epoch: 40, Loss: 0.625, Data Loss: 0.064, Symnet Regularize: 3.743\n",
      "[PRINT] Epoch: 50, Loss: 0.624, Data Loss: 0.064, Symnet Regularize: 3.735\n",
      "[PRINT] Epoch: 60, Loss: 0.623, Data Loss: 0.064, Symnet Regularize: 3.728\n",
      "[PRINT] Epoch: 70, Loss: 0.623, Data Loss: 0.064, Symnet Regularize: 3.721\n",
      "[PRINT] Epoch: 80, Loss: 0.622, Data Loss: 0.065, Symnet Regularize: 3.715\n",
      "[PRINT] Epoch: 90, Loss: 0.621, Data Loss: 0.065, Symnet Regularize: 3.708\n",
      "[PRINT] Epoch: 100, Loss: 0.621, Data Loss: 0.065, Symnet Regularize: 3.702\n",
      "[PRINT] Epoch: 110, Loss: 0.620, Data Loss: 0.066, Symnet Regularize: 3.695\n",
      "[PRINT] Epoch: 120, Loss: 0.619, Data Loss: 0.066, Symnet Regularize: 3.689\n",
      "[PRINT] Epoch: 130, Loss: 0.619, Data Loss: 0.066, Symnet Regularize: 3.683\n",
      "[PRINT] Epoch: 140, Loss: 0.618, Data Loss: 0.067, Symnet Regularize: 3.677\n",
      "[PRINT] Epoch: 150, Loss: 0.618, Data Loss: 0.067, Symnet Regularize: 3.672\n",
      "[PRINT] Epoch: 160, Loss: 0.617, Data Loss: 0.067, Symnet Regularize: 3.666\n",
      "[PRINT] Epoch: 170, Loss: 0.617, Data Loss: 0.068, Symnet Regularize: 3.661\n",
      "[PRINT] Epoch: 180, Loss: 0.616, Data Loss: 0.068, Symnet Regularize: 3.656\n",
      "[PRINT] Epoch: 190, Loss: 0.616, Data Loss: 0.068, Symnet Regularize: 3.651\n",
      "[PRINT] Epoch: 200, Loss: 0.615, Data Loss: 0.069, Symnet Regularize: 3.646\n",
      "[PRINT] Epoch: 210, Loss: 0.615, Data Loss: 0.069, Symnet Regularize: 3.641\n",
      "[PRINT] Epoch: 220, Loss: 0.615, Data Loss: 0.069, Symnet Regularize: 3.636\n",
      "[PRINT] Epoch: 230, Loss: 0.614, Data Loss: 0.069, Symnet Regularize: 3.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 240, Loss: 0.614, Data Loss: 0.070, Symnet Regularize: 3.627\n",
      "[PRINT] Epoch: 250, Loss: 0.613, Data Loss: 0.070, Symnet Regularize: 3.622\n",
      "[PRINT] Epoch: 260, Loss: 0.613, Data Loss: 0.070, Symnet Regularize: 3.618\n",
      "[PRINT] Epoch: 270, Loss: 0.613, Data Loss: 0.071, Symnet Regularize: 3.613\n",
      "[PRINT] Epoch: 280, Loss: 0.612, Data Loss: 0.071, Symnet Regularize: 3.609\n",
      "[PRINT] Epoch: 290, Loss: 0.612, Data Loss: 0.071, Symnet Regularize: 3.605\n",
      "[PRINT] Epoch: 300, Loss: 0.611, Data Loss: 0.071, Symnet Regularize: 3.600\n",
      "[PRINT] Epoch: 310, Loss: 0.611, Data Loss: 0.072, Symnet Regularize: 3.596\n",
      "[PRINT] Epoch: 320, Loss: 0.611, Data Loss: 0.072, Symnet Regularize: 3.591\n",
      "[PRINT] Epoch: 330, Loss: 0.610, Data Loss: 0.072, Symnet Regularize: 3.587\n",
      "[PRINT] Epoch: 340, Loss: 0.610, Data Loss: 0.073, Symnet Regularize: 3.582\n",
      "[PRINT] Epoch: 350, Loss: 0.609, Data Loss: 0.073, Symnet Regularize: 3.578\n",
      "[PRINT] Epoch: 360, Loss: 0.609, Data Loss: 0.073, Symnet Regularize: 3.573\n",
      "[PRINT] Epoch: 370, Loss: 0.609, Data Loss: 0.073, Symnet Regularize: 3.568\n",
      "[PRINT] Epoch: 380, Loss: 0.608, Data Loss: 0.074, Symnet Regularize: 3.563\n",
      "[PRINT] Epoch: 390, Loss: 0.608, Data Loss: 0.074, Symnet Regularize: 3.559\n",
      "[PRINT] Epoch: 400, Loss: 0.607, Data Loss: 0.074, Symnet Regularize: 3.554\n",
      "[PRINT] Epoch: 410, Loss: 0.607, Data Loss: 0.075, Symnet Regularize: 3.549\n",
      "[PRINT] Epoch: 420, Loss: 0.607, Data Loss: 0.075, Symnet Regularize: 3.543\n",
      "[PRINT] Epoch: 430, Loss: 0.606, Data Loss: 0.075, Symnet Regularize: 3.538\n",
      "[PRINT] Epoch: 440, Loss: 0.606, Data Loss: 0.076, Symnet Regularize: 3.533\n",
      "[PRINT] Epoch: 450, Loss: 0.605, Data Loss: 0.076, Symnet Regularize: 3.527\n",
      "[PRINT] Epoch: 460, Loss: 0.605, Data Loss: 0.077, Symnet Regularize: 3.521\n",
      "[PRINT] Epoch: 470, Loss: 0.604, Data Loss: 0.077, Symnet Regularize: 3.516\n",
      "[PRINT] Epoch: 480, Loss: 0.604, Data Loss: 0.077, Symnet Regularize: 3.509\n",
      "[PRINT] Epoch: 490, Loss: 0.603, Data Loss: 0.078, Symnet Regularize: 3.503\n",
      "[PRINT] Epoch: 500, Loss: 0.603, Data Loss: 0.078, Symnet Regularize: 3.497\n",
      "[PRINT] Epoch: 510, Loss: 0.602, Data Loss: 0.079, Symnet Regularize: 3.490\n",
      "[PRINT] Epoch: 520, Loss: 0.602, Data Loss: 0.079, Symnet Regularize: 3.483\n",
      "[PRINT] Epoch: 530, Loss: 0.601, Data Loss: 0.080, Symnet Regularize: 3.475\n",
      "[PRINT] Epoch: 540, Loss: 0.600, Data Loss: 0.080, Symnet Regularize: 3.467\n",
      "[PRINT] Epoch: 550, Loss: 0.600, Data Loss: 0.081, Symnet Regularize: 3.459\n",
      "[PRINT] Epoch: 560, Loss: 0.599, Data Loss: 0.081, Symnet Regularize: 3.450\n",
      "[PRINT] Epoch: 570, Loss: 0.598, Data Loss: 0.082, Symnet Regularize: 3.441\n",
      "[PRINT] Epoch: 580, Loss: 0.597, Data Loss: 0.083, Symnet Regularize: 3.431\n",
      "[PRINT] Epoch: 590, Loss: 0.597, Data Loss: 0.083, Symnet Regularize: 3.421\n",
      "[PRINT] Epoch: 600, Loss: 0.596, Data Loss: 0.084, Symnet Regularize: 3.410\n",
      "[PRINT] Epoch: 610, Loss: 0.595, Data Loss: 0.085, Symnet Regularize: 3.403\n",
      "[PRINT] Epoch: 620, Loss: 0.594, Data Loss: 0.085, Symnet Regularize: 3.396\n",
      "[PRINT] Epoch: 630, Loss: 0.593, Data Loss: 0.085, Symnet Regularize: 3.390\n",
      "[PRINT] Epoch: 640, Loss: 0.593, Data Loss: 0.085, Symnet Regularize: 3.383\n",
      "[PRINT] Epoch: 650, Loss: 0.592, Data Loss: 0.086, Symnet Regularize: 3.376\n",
      "[PRINT] Epoch: 660, Loss: 0.591, Data Loss: 0.086, Symnet Regularize: 3.369\n",
      "[PRINT] Epoch: 670, Loss: 0.591, Data Loss: 0.086, Symnet Regularize: 3.362\n",
      "[PRINT] Epoch: 680, Loss: 0.590, Data Loss: 0.087, Symnet Regularize: 3.355\n",
      "[PRINT] Epoch: 690, Loss: 0.589, Data Loss: 0.087, Symnet Regularize: 3.348\n",
      "[PRINT] Epoch: 700, Loss: 0.588, Data Loss: 0.087, Symnet Regularize: 3.341\n",
      "[PRINT] Epoch: 710, Loss: 0.588, Data Loss: 0.088, Symnet Regularize: 3.334\n",
      "[PRINT] Epoch: 720, Loss: 0.587, Data Loss: 0.088, Symnet Regularize: 3.326\n",
      "[PRINT] Epoch: 730, Loss: 0.586, Data Loss: 0.088, Symnet Regularize: 3.319\n",
      "[PRINT] Epoch: 740, Loss: 0.585, Data Loss: 0.089, Symnet Regularize: 3.311\n",
      "[PRINT] Epoch: 750, Loss: 0.585, Data Loss: 0.089, Symnet Regularize: 3.304\n",
      "[PRINT] Epoch: 760, Loss: 0.584, Data Loss: 0.089, Symnet Regularize: 3.296\n",
      "[PRINT] Epoch: 770, Loss: 0.583, Data Loss: 0.090, Symnet Regularize: 3.287\n",
      "[PRINT] Epoch: 780, Loss: 0.582, Data Loss: 0.090, Symnet Regularize: 3.281\n",
      "[PRINT] Epoch: 790, Loss: 0.582, Data Loss: 0.090, Symnet Regularize: 3.275\n",
      "[PRINT] Epoch: 800, Loss: 0.581, Data Loss: 0.090, Symnet Regularize: 3.270\n",
      "[PRINT] Epoch: 810, Loss: 0.580, Data Loss: 0.091, Symnet Regularize: 3.265\n",
      "[PRINT] Epoch: 820, Loss: 0.580, Data Loss: 0.091, Symnet Regularize: 3.259\n",
      "[PRINT] Epoch: 830, Loss: 0.579, Data Loss: 0.091, Symnet Regularize: 3.254\n",
      "[PRINT] Epoch: 840, Loss: 0.578, Data Loss: 0.091, Symnet Regularize: 3.249\n",
      "[PRINT] Epoch: 850, Loss: 0.578, Data Loss: 0.091, Symnet Regularize: 3.244\n",
      "[PRINT] Epoch: 860, Loss: 0.577, Data Loss: 0.091, Symnet Regularize: 3.240\n",
      "[PRINT] Epoch: 870, Loss: 0.576, Data Loss: 0.091, Symnet Regularize: 3.235\n",
      "[PRINT] Epoch: 880, Loss: 0.576, Data Loss: 0.091, Symnet Regularize: 3.230\n",
      "[PRINT] Epoch: 890, Loss: 0.575, Data Loss: 0.091, Symnet Regularize: 3.225\n",
      "[PRINT] Epoch: 900, Loss: 0.574, Data Loss: 0.091, Symnet Regularize: 3.221\n",
      "[PRINT] Epoch: 910, Loss: 0.574, Data Loss: 0.091, Symnet Regularize: 3.216\n",
      "[PRINT] Epoch: 920, Loss: 0.573, Data Loss: 0.091, Symnet Regularize: 3.211\n",
      "[PRINT] Epoch: 930, Loss: 0.572, Data Loss: 0.092, Symnet Regularize: 3.206\n",
      "[PRINT] Epoch: 940, Loss: 0.572, Data Loss: 0.092, Symnet Regularize: 3.201\n",
      "[PRINT] Epoch: 950, Loss: 0.571, Data Loss: 0.092, Symnet Regularize: 3.197\n",
      "[PRINT] Epoch: 960, Loss: 0.570, Data Loss: 0.092, Symnet Regularize: 3.192\n",
      "[PRINT] Epoch: 970, Loss: 0.570, Data Loss: 0.092, Symnet Regularize: 3.187\n",
      "[PRINT] Epoch: 980, Loss: 0.569, Data Loss: 0.092, Symnet Regularize: 3.182\n",
      "[PRINT] Epoch: 990, Loss: 0.568, Data Loss: 0.092, Symnet Regularize: 3.177\n",
      "[PRINT] block: 4\n",
      "[PRINT] Epoch: 0, Loss: 0.783, Data Loss: 0.149, Symnet Regularize: 3.172\n",
      "[PRINT] Epoch: 10, Loss: 0.791, Data Loss: 0.157, Symnet Regularize: 3.169\n",
      "[PRINT] Epoch: 20, Loss: 0.797, Data Loss: 0.165, Symnet Regularize: 3.158\n",
      "[PRINT] Epoch: 30, Loss: 0.783, Data Loss: 0.152, Symnet Regularize: 3.156\n",
      "[PRINT] Epoch: 40, Loss: 0.779, Data Loss: 0.150, Symnet Regularize: 3.149\n",
      "[PRINT] Epoch: 50, Loss: 0.777, Data Loss: 0.149, Symnet Regularize: 3.140\n",
      "[PRINT] Epoch: 60, Loss: 0.776, Data Loss: 0.149, Symnet Regularize: 3.135\n",
      "[PRINT] Epoch: 70, Loss: 0.775, Data Loss: 0.149, Symnet Regularize: 3.132\n",
      "[PRINT] Epoch: 80, Loss: 0.774, Data Loss: 0.149, Symnet Regularize: 3.128\n",
      "[PRINT] Epoch: 90, Loss: 0.774, Data Loss: 0.149, Symnet Regularize: 3.125\n",
      "[PRINT] Epoch: 100, Loss: 0.773, Data Loss: 0.149, Symnet Regularize: 3.122\n",
      "[PRINT] Epoch: 110, Loss: 0.773, Data Loss: 0.149, Symnet Regularize: 3.119\n",
      "[PRINT] Epoch: 120, Loss: 0.772, Data Loss: 0.149, Symnet Regularize: 3.116\n",
      "[PRINT] Epoch: 130, Loss: 0.771, Data Loss: 0.149, Symnet Regularize: 3.113\n",
      "[PRINT] Epoch: 140, Loss: 0.771, Data Loss: 0.149, Symnet Regularize: 3.110\n",
      "[PRINT] Epoch: 150, Loss: 0.770, Data Loss: 0.149, Symnet Regularize: 3.106\n",
      "[PRINT] Epoch: 160, Loss: 0.769, Data Loss: 0.149, Symnet Regularize: 3.103\n",
      "[PRINT] Epoch: 170, Loss: 0.769, Data Loss: 0.149, Symnet Regularize: 3.100\n",
      "[PRINT] Epoch: 180, Loss: 0.768, Data Loss: 0.149, Symnet Regularize: 3.097\n",
      "[PRINT] Epoch: 190, Loss: 0.767, Data Loss: 0.149, Symnet Regularize: 3.094\n",
      "[PRINT] Epoch: 200, Loss: 0.767, Data Loss: 0.149, Symnet Regularize: 3.090\n",
      "[PRINT] Epoch: 210, Loss: 0.766, Data Loss: 0.149, Symnet Regularize: 3.087\n",
      "[PRINT] Epoch: 220, Loss: 0.765, Data Loss: 0.149, Symnet Regularize: 3.084\n",
      "[PRINT] Epoch: 230, Loss: 0.765, Data Loss: 0.149, Symnet Regularize: 3.080\n",
      "[PRINT] Epoch: 240, Loss: 0.764, Data Loss: 0.149, Symnet Regularize: 3.077\n",
      "[PRINT] Epoch: 250, Loss: 0.763, Data Loss: 0.149, Symnet Regularize: 3.074\n",
      "[PRINT] Epoch: 260, Loss: 0.763, Data Loss: 0.149, Symnet Regularize: 3.070\n",
      "[PRINT] Epoch: 270, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.067\n",
      "[PRINT] Epoch: 280, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.065\n",
      "[PRINT] Epoch: 290, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 300, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 310, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 320, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 330, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 340, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 350, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 360, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 370, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 380, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 390, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 400, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 410, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 420, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 430, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 440, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 450, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 460, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 470, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 480, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 490, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 500, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 510, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 520, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 530, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 540, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 550, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 560, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 570, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 580, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 590, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 600, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 610, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 620, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 630, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 640, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 650, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 660, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 670, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 680, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 690, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 700, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 710, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 720, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 730, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 740, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 750, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 760, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 770, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 780, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 790, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 800, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 810, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 820, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 830, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 840, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 850, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 860, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 870, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 880, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 890, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 900, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 910, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 920, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 930, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 940, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 950, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 960, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 970, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 980, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 990, Loss: 0.762, Data Loss: 0.149, Symnet Regularize: 3.064\n",
      "[PRINT] block: 5\n",
      "[PRINT] Epoch: 0, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 10, Loss: 1.138, Data Loss: 0.372, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 20, Loss: 1.047, Data Loss: 0.280, Symnet Regularize: 3.067\n",
      "[PRINT] Epoch: 30, Loss: 0.991, Data Loss: 0.225, Symnet Regularize: 3.065\n",
      "[PRINT] Epoch: 40, Loss: 0.995, Data Loss: 0.229, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 50, Loss: 0.991, Data Loss: 0.225, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 60, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 70, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 80, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 90, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 100, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 110, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 120, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 130, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 140, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 150, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 160, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 170, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 180, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 190, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 200, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 210, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 220, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 230, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 240, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 250, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 260, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 270, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 280, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 290, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 300, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 310, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 320, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 330, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 340, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 350, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 360, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 370, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 380, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 390, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 400, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 410, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 420, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 430, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 440, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 450, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 460, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 470, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 480, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 490, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 500, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 510, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 520, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 530, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 540, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 550, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 560, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 570, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 580, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 590, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 600, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 610, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 620, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 630, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 640, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 650, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 660, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 670, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 680, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 690, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 700, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 710, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 720, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 730, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 740, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 750, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 760, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 770, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 780, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 790, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 800, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 810, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 820, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 830, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 840, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 850, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 860, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 870, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 880, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 890, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 900, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 910, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 920, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 930, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 940, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 950, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 960, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 970, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 980, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 990, Loss: 0.990, Data Loss: 0.224, Symnet Regularize: 3.064\n",
      "[PRINT] block: 6\n",
      "[PRINT] Epoch: 0, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 10, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 20, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 30, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 40, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 50, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 60, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 70, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 80, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 90, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 100, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 110, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 120, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 130, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 140, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 150, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 160, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 170, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 180, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 190, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 200, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 210, Loss: 1.254, Data Loss: 0.335, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 220, Loss: 1.244, Data Loss: 0.324, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 230, Loss: 1.243, Data Loss: 0.324, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 240, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 250, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 260, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 270, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 280, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 290, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 300, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 310, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 320, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 330, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 340, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 350, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 360, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 370, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 380, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 390, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 400, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 410, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 420, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 430, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 440, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 450, Loss: 1.256, Data Loss: 0.337, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 460, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 470, Loss: 1.243, Data Loss: 0.324, Symnet Regularize: 3.063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 480, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 490, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 500, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 510, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 520, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 530, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 540, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 550, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 560, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 570, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 580, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 590, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 600, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 610, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 620, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 630, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 640, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 650, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 660, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 670, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 680, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 690, Loss: 1.244, Data Loss: 0.325, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 700, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 710, Loss: 1.243, Data Loss: 0.324, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 720, Loss: 1.243, Data Loss: 0.324, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 730, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 740, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 750, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 760, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 770, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 780, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 790, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 800, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 810, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 820, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 830, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 840, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 850, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 860, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 870, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 880, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 890, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 900, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 910, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 920, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 930, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 940, Loss: 1.242, Data Loss: 0.324, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 950, Loss: 1.248, Data Loss: 0.329, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 960, Loss: 1.244, Data Loss: 0.325, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 970, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 980, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 990, Loss: 1.242, Data Loss: 0.323, Symnet Regularize: 3.063\n",
      "[PRINT] block: 7\n",
      "[PRINT] Epoch: 0, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 10, Loss: 1.533, Data Loss: 0.461, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 20, Loss: 1.538, Data Loss: 0.466, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 30, Loss: 1.566, Data Loss: 0.493, Symnet Regularize: 3.064\n",
      "[PRINT] Epoch: 40, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 50, Loss: 1.531, Data Loss: 0.459, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 60, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 70, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 80, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 90, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 100, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 110, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 120, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 130, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 140, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 150, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 160, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 170, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 180, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 190, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 200, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 210, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 220, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 230, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 240, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 250, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 260, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 270, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 280, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 290, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 300, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 310, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 320, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 330, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 340, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 350, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 360, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 370, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 380, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 390, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 400, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 410, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 420, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 430, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 440, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 450, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 460, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 470, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 480, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 490, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 500, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 510, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 520, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 530, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 540, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 550, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 560, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 570, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 580, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 590, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 600, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 610, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 620, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 630, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 640, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 650, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 660, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 670, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 680, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 690, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 700, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 710, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 720, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 730, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 740, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 750, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 760, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 770, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 780, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 790, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 800, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 810, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 820, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 830, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 840, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 850, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 860, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 870, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 880, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 890, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 900, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 910, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 920, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 930, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 940, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 950, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 960, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 970, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 980, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 990, Loss: 1.527, Data Loss: 0.455, Symnet Regularize: 3.062\n",
      "[PRINT] block: 8\n",
      "[PRINT] Epoch: 0, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 10, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 20, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 30, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 40, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 50, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 60, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 70, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 80, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 90, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 100, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 110, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 120, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 130, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 140, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 150, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 160, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 170, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 180, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 190, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 200, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 210, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 220, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 230, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 240, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 250, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 260, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 270, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 280, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 290, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 300, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 310, Loss: 1.865, Data Loss: 0.640, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 320, Loss: 1.869, Data Loss: 0.645, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 330, Loss: 1.859, Data Loss: 0.635, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 340, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 350, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 360, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 370, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 380, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 390, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 400, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 410, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 420, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 430, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 440, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 450, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 460, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 470, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 480, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 490, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 500, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 510, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 520, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 530, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 540, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 550, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 560, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 570, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 580, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 590, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 600, Loss: 1.876, Data Loss: 0.652, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 610, Loss: 1.867, Data Loss: 0.642, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 620, Loss: 1.862, Data Loss: 0.637, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 630, Loss: 1.859, Data Loss: 0.634, Symnet Regularize: 3.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 640, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 650, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 660, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 670, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 680, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 690, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 700, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 710, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 720, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 730, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 740, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 750, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 760, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 770, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 780, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 790, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 800, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 810, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 820, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 830, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 840, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 850, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 860, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 870, Loss: 1.858, Data Loss: 0.634, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 880, Loss: 1.882, Data Loss: 0.658, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 890, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 900, Loss: 1.859, Data Loss: 0.635, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 910, Loss: 1.858, Data Loss: 0.634, Symnet Regularize: 3.062\n",
      "[PRINT] Epoch: 920, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 930, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 940, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 950, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 960, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 970, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 980, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 990, Loss: 1.858, Data Loss: 0.633, Symnet Regularize: 3.061\n",
      "[PRINT] block: 9\n",
      "[PRINT] Epoch: 0, Loss: 2.253, Data Loss: 0.875, Symnet Regularize: 3.061\n",
      "[PRINT] Epoch: 10, Loss: 2.350, Data Loss: 0.974, Symnet Regularize: 3.059\n",
      "[PRINT] Epoch: 20, Loss: 2.256, Data Loss: 0.879, Symnet Regularize: 3.059\n",
      "[PRINT] Epoch: 30, Loss: 2.322, Data Loss: 0.943, Symnet Regularize: 3.063\n",
      "[PRINT] Epoch: 40, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 50, Loss: 2.260, Data Loss: 0.884, Symnet Regularize: 3.059\n",
      "[PRINT] Epoch: 60, Loss: 2.253, Data Loss: 0.876, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 70, Loss: 2.253, Data Loss: 0.876, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 80, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 90, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 100, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 110, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 120, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 130, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 140, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 150, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 160, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 170, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 180, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 190, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 200, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 210, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 220, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 230, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 240, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 250, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 260, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 270, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 280, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 290, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 300, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 310, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 320, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 330, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 340, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 350, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 360, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 370, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 380, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 390, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 400, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 410, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 420, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 430, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 440, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 450, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 460, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 470, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 480, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 490, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 500, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 510, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 520, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 530, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 540, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 550, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 560, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 570, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 580, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 590, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 600, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 610, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 620, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 630, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 640, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 650, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 660, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 670, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 680, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 690, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 700, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 710, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 720, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 730, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 740, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 750, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 760, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 770, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 780, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 790, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 800, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 810, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 820, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 830, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 840, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 850, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 860, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 870, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 880, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 890, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 900, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 910, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 920, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 930, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 940, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 950, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 960, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 970, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 980, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 990, Loss: 2.252, Data Loss: 0.875, Symnet Regularize: 3.060\n",
      "[PRINT] block: 10\n",
      "[PRINT] Epoch: 0, Loss: 2.736, Data Loss: 1.206, Symnet Regularize: 3.060\n",
      "[PRINT] Epoch: 10, Loss: 2.736, Data Loss: 1.207, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 20, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 30, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.059\n",
      "[PRINT] Epoch: 40, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 50, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 60, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 70, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 80, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 90, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 100, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 110, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 120, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 130, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 140, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 150, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 160, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n",
      "[PRINT] Epoch: 170, Loss: 2.735, Data Loss: 1.206, Symnet Regularize: 3.058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1042f3768439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print(\"------------++++++++++++++++++++++++++++\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_norm_(model.parameters(),0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         print(\"------------++++++++++++++++++++++++++++\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    u_obs = data_model.data(stepnum+1) #np array of stepnum elements\n",
    "    for epoch in range(1000):\n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        loss,data_loss,syment_reg = modelLoss(model,u_obs,config,block)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f\" % (epoch,loss,\\\n",
    "                                                                                              data_loss,syment_reg))\n",
    "\n",
    "    \n",
    "# put prints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load('burgers_weight.pth')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.974806 u_{0} u_{1} - 0.00462748 u_{0} u_{2} - 0.00480364 u_{0} - 0.00430956 u_{1}^{2} + 0.100894 u_{2} - 0.00235616$"
      ],
      "text/plain": [
       "-0.974806*u_0*u_1 - 0.00462748*u_0*u_2 - 0.00480364*u_0 - 0.00430956*u_1**2 + 0.100894*u_2 - 0.00235616"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet.getEquation(calprec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('symnet.layer0.weight',\n",
       "              tensor([[-3.1466e-41,  4.3728e-38, -2.7045e-40],\n",
       "                      [ 7.1718e-42,  1.4293e-42, -1.9632e-42]])),\n",
       "             ('symnet.layer0.bias', tensor([1.8190e-28, 1.3788e-13])),\n",
       "             ('symnet.layer1.weight',\n",
       "              tensor([[ 9.9174e-01,  4.3844e-03,  6.0919e-04,  2.6048e-36],\n",
       "                      [-7.0374e-04,  9.9059e-01,  4.7024e-03,  5.1386e-34]])),\n",
       "             ('symnet.layer1.bias', tensor([1.2057e-05, 2.3986e-03])),\n",
       "             ('symnet.layer_final.weight',\n",
       "              tensor([[-2.4433e-03, -5.5761e-05,  1.0089e-01,  9.9048e-35, -9.9226e-01]])),\n",
       "             ('symnet.layer_final.bias', tensor([-0.0024]))])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Additional Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model.symnet.layer0.weight.data.cpu().numpy()\n",
    "w1\n",
    "w1_d = sympy.Matrix(w1)\n",
    "w1_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _cast2symbol(layer):\n",
    "    weight,bias = layer.weight.data.cpu().numpy(), \\\n",
    "                layer.bias.data.cpu().numpy()\n",
    "    weight,bias = sympy.Matrix(weight),sympy.Matrix(bias)\n",
    "    return weight,bias\n",
    "\n",
    "def _sympychop(o, calprec):\n",
    "    for i in range(o.shape[0]):\n",
    "        cdict = o[i].expand().as_coefficients_dict()  \n",
    "        o_i = 0\n",
    "        for k,v in cdict.items():\n",
    "            if abs(v)>0.1**calprec:\n",
    "                o_i = o_i+k*v\n",
    "        o[i] = o_i\n",
    "    return o\n",
    "\n",
    "def getEquation(model,calprec=6):\n",
    "    ## assume symnet model\n",
    "    \n",
    "    deriv_channels = sympy.symbols(model.channel_names)\n",
    "    deriv_channels = sympy.Matrix([deriv_channels,])\n",
    "    for i in range(model.n_hidden):\n",
    "        weight,bias = _cast2symbol(model.layers[i])\n",
    "        o = weight*deriv_channels.transpose()+bias\n",
    "        o = _sympychop(o, calprec) #ignores very low params terms\n",
    "        deriv_channels = list(deriv_channels)+[o[0]*o[1],]\n",
    "        deriv_channels = sympy.Matrix([deriv_channels,])\n",
    "        \n",
    "    weight,bias = _cast2symbol(model.layers[-1])\n",
    "    o = (weight*deriv_channels.transpose()+bias)\n",
    "    o = _sympychop(o,calprec)\n",
    "\n",
    "    return o[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.974806 u u_{x} + 0.100894 u_{xx}$"
      ],
      "text/plain": [
       "-0.974806*u*u_x + 0.100894*u_xx"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEquation(model.symnet,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.symnet.channel_names = ['u','u_x','u_xx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0. ,  0.5])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKernel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Differences With Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FD1DMoment(torch.nn.Module):\n",
    "    '''\n",
    "        Finite Differences scheme for 1D dependency\n",
    "        acc_order list of acc_order for diff_order\n",
    "        kernel size should be greater than or equal to the max_diff_order (otherwise error will be thrown)\n",
    "        diff_order starts from 0,1,... \n",
    "        constarint: Moment matrix constraint \"free\" or \"moment\"\n",
    "        Basically this class initializes one kernel of the specified parameters\n",
    "    '''\n",
    "    def __init__(self,dx, kernel_size, diff_order,acc_order,constraint='free'):\n",
    "        super(FD1DMoment, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.kernel_size = kernel_size\n",
    "        self.diff_order = diff_order\n",
    "        self.acc_order = acc_order\n",
    "        self.constraint = constraint\n",
    "        ## will only be used in case of contraint = 'free'\n",
    "        if constraint=='free':\n",
    "            self._kernel = (getKernelTorch(diff_order,acc_order+1,dim=kernel_size,scheme='central')/(dx**diff_order)).type(torch.DoubleTensor)\n",
    "            \n",
    "        ##Moment to kernel and vice versa\n",
    "        if constraint=='moment':\n",
    "            self.m2k = M2K(kernel_size)\n",
    "            self.k2m = K2M(kernel_size)\n",
    "            #define moment matrix\n",
    "            moment = torch.DoubleTensor(kernel_size).zero_()\n",
    "            moment[diff_order] = 1\n",
    "            moment = moment.reshape(1,1,-1)\n",
    "            self.moment = torch.nn.Parameter(moment) ## now weights will be updated on this\n",
    "            ##create a mask for gradeint hook\n",
    "            self.gradient_mask = self._getGradientMask()\n",
    "            ##register hook to the moment matrix\n",
    "            self.moment.register_hook(lambda grad: grad.mul_(self.gradient_mask))\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def kernel(self):\n",
    "        if self.constraint == 'moment':\n",
    "            kernel = self.m2k(self.moment)/(self.dx**self.diff_order)\n",
    "        else:\n",
    "            kernel = self._kernel\n",
    "            \n",
    "            \n",
    "        return kernel\n",
    "    \n",
    "    def _getGradientMask(self):\n",
    "        gradient_mask = torch.ones(self.kernel_size,dtype=torch.double)\n",
    "        order_bank = np.arange(self.kernel_size)\n",
    "        for j in range(self.diff_order+self.acc_order):\n",
    "            gradient_mask[order_bank[j]] = 0\n",
    "        gradient_mask = gradient_mask.reshape(1,1,-1)\n",
    "        return gradient_mask\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        '''\n",
    "            Process:\n",
    "            Need to pad the input and then appy conv1D\n",
    "            input shape can be batch_size x n_channels x x_dim\n",
    "        '''\n",
    "        inp_padded = padInputTorch(inputs,self.diff_order,self.acc_order+1,dim=self.kernel_size) #batch_size x n_channels x (x_dim+padded)\n",
    "#         print(inp_padded)\n",
    "        conv = F.conv1d(inp_padded.type(torch.DoubleTensor),self.kernel)\n",
    "        return conv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 40])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1,1,40)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6544,  0.3960, -0.4965,  0.1895, -0.2412, -0.3051,  0.6687,\n",
       "          -0.2338, -0.2105,  0.6703, -0.7742, -0.7891,  1.2694, -0.5749,\n",
       "          -0.6582,  0.9995,  0.7799,  0.5030,  0.2730, -0.6519, -0.5912,\n",
       "           0.0077, -0.6355, -0.7861,  0.5252, -0.4820, -0.4739,  0.5875,\n",
       "           0.6501, -0.1029, -0.8475, -0.6698,  0.9173,  1.5599, -0.7804,\n",
       "           0.5820, -0.0037, -1.2849,  0.8986,  1.6743]]], dtype=torch.float64,\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FD1DMoment(1,3,1,2,constraint='moment') #fix nans in accuracy order > 2\n",
    "fd(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[0., 1., 0.]]], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fd.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.2352e-02,  7.8054e-03,  3.2451e-02,  4.2015e-01,  4.5650e-02,\n",
       "           3.2077e-02,  2.6063e-01, -8.3255e-01,  3.2625e-01,  1.6310e+00,\n",
       "           3.0084e-01, -1.3547e+00, -7.3103e-01,  6.2955e-01,  1.3862e-01,\n",
       "          -5.9324e-02, -3.9002e-01,  2.9395e-01,  8.2532e-01,  2.5238e-01,\n",
       "           5.8228e-01, -1.3900e+00, -1.3439e+00,  4.2385e-01,  2.6559e-01,\n",
       "           1.2791e+00,  3.8773e-01, -2.0274e+00, -6.3823e-01,  2.4442e-01,\n",
       "           2.9626e-01,  8.5085e-01,  1.5764e-04, -4.5245e-01,  3.2517e-01,\n",
       "           7.6670e-01,  1.6585e-01, -6.8229e-01,  2.0467e-01,  5.6886e+00]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FD1D(1,3,1,2,constraint='free') #fix nans in accuracy order > 2\n",
    "fd(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying PdeNet with Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdeNet(torch.nn.Module):\n",
    "    def __init__(self,dt, dx, kernel_size, max_diff_order, n_channel,channel_names,acc_order=2,n_hidden=2,\\\n",
    "                constraint='free'):\n",
    "        '''\n",
    "        Input:\n",
    "        '''\n",
    "        super(PdeNet, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dt = dt\n",
    "        self.kernel_size = kernel_size\n",
    "        self.max_diff_order = max_diff_order\n",
    "        self.n_channel = n_channel\n",
    "        self.channel_names = channel_names\n",
    "        self.n_hidden = n_hidden\n",
    "        self.constraint = constraint\n",
    "                        \n",
    "        if not np.iterable(acc_order):\n",
    "            acc_order = [acc_order,]*(self.max_diff_order+1)\n",
    "            \n",
    "        self.acc_order = acc_order\n",
    "        \n",
    "        #conv operation\n",
    "        for i in range(max_diff_order+1):\n",
    "            kernel = FD1DMoment(dx,kernel_size,i,acc_order[i],constraint=constraint)\n",
    "            self.add_module('fd'+str(i), kernel) #finite difference of order\n",
    "            \n",
    "        #symnet \n",
    "        c = channel_names.split(',')\n",
    "        derivative_channels = []\n",
    "        for ch in c:\n",
    "            for k in range(max_diff_order+1):\n",
    "                derivative_channels.append(ch+'_'+str(k))\n",
    "        self.derivative_channels = derivative_channels \n",
    "        self.add_module(\"symnet\",SymNet(n_hidden,len(derivative_channels), deriv_channel_names=derivative_channels))\n",
    "    \n",
    "    @property\n",
    "    def fds(self):\n",
    "        for i in range(self.max_diff_order+1):\n",
    "            yield self.__getattr__('fd'+str(i))\n",
    "                \n",
    "    def multistep(self,inputs,step_num):\n",
    "        #pass it throught the kernels then the symmnet to \n",
    "        '''\n",
    "        Takes multistep through the whole PDE Net.\n",
    "        '''\n",
    "        u = inputs\n",
    "        for i in range(step_num):\n",
    "            uadd = self.RightHandItems(u)#will take a dt step from u using the network\n",
    "            u = u + self.dt*uadd\n",
    "        return u\n",
    "    \n",
    "    def diffParams(self):\n",
    "        params = []\n",
    "        for fd in self.fds:\n",
    "            params += list(fd.parameters())\n",
    "        return params\n",
    "\n",
    "    def RightHandItems(self,u):\n",
    "        \n",
    "        #convolve the u with the derivative kernals to get the different derivatives \n",
    "        #batch_size x n_channels x X_dim\n",
    "        u_derives = []\n",
    "        for i in range(self.max_diff_order+1):\n",
    "            fd_obj = self.__getattr__('fd'+str(i))\n",
    "            u_deriv_order_i = fd_obj(u)\n",
    "            \n",
    "            u_derives.append(u_deriv_order_i)\n",
    "            \n",
    "        u = torch.cat(u_derives, dim=1) #batch_size x n_derivatives x X_dim \n",
    "        #symnet_output = (batch_size x X_dim x n_derivatives)\n",
    "        symnet = self.__getattr__('symnet')\n",
    "        u_symnet = symnet(u.permute(0,2,1)) #batch_size x X_dim\n",
    "        u_out = u_symnet.unsqueeze_(1)\n",
    "        return u_out\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs,step_num):\n",
    "        '''\n",
    "            inputs of shape batch_size x n_channels(1 for our case) x X_dim\n",
    "            step_nums = number of dt blocks to calculate the inputs for\n",
    "        '''\n",
    "        return self.multistep(inputs,step_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symnetRegularizeLoss(model):\n",
    "    loss = 0\n",
    "    s = 1e-2\n",
    "    for p in model.symnet.parameters():\n",
    "        p = p.abs()\n",
    "        loss = loss+((p<s).to(p)*0.5/s*p**2).sum()+((p>=s).to(p)*(p-s/2)).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentRegularizeLoss(model):\n",
    "    loss = 0\n",
    "    s = 1e-2\n",
    "    for p in model.diffParams():\n",
    "        p = p.abs()\n",
    "        loss = loss+((p<s).to(p)*0.5/s*p**2).sum()+((p>=s).to(p)*(p-s/2)).sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global names are all the parameters\n",
    "def modelLoss(model,u_obs,config,block):\n",
    "    '''\n",
    "        Returns the loss value for so that it can be given to an optimizer\n",
    "        Inputs:\n",
    "            u_obs (batch_size x n_channels x X_dim)\n",
    "            blocks is stepnum\n",
    "    '''\n",
    "    sparsity = config['sparsity']\n",
    "    momentsparsity = config['momentsparsity']\n",
    "    \n",
    "    \n",
    "    if block==0: #warmup\n",
    "        sparsity = 0\n",
    "    step_num = block if block>=1 else 1\n",
    "    dt = config['dt']\n",
    "    data_loss = 0\n",
    "    symnet_loss = symnetRegularizeLoss(model)\n",
    "    moment_loss = momentRegularizeLoss(model)\n",
    "    ut = u_obs[0]\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    for steps in range(1,step_num+1):\n",
    "        ut_next_predicted = model(ut,step_num=1) #take one step from this point\n",
    "        data_loss += (mse_loss(ut_next_predicted,u_obs[steps])/dt**2)/step_num\n",
    "        ut = ut_next_predicted\n",
    "\n",
    "    loss = data_loss+stepnum*sparsity*symnet_loss+stepnum*momentsparsity*moment_loss\n",
    "    if torch.isnan(loss):\n",
    "        raise \"Loss Nan\"\n",
    "        loss = (torch.ones(1,requires_grad=True)/torch.zeros(1)).to(loss)\n",
    "    return loss,data_loss,symnet_loss,moment_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "##modify channel names and length\n",
    "def setenv(config): #return model and datamodel\n",
    "    model = PdeNet(config['dt'],config['dx'],config['kernel_size'],config['max_diff_order']\\\n",
    "                   ,1,config['channel_names'],config['acc_order'],config['n_hidden_layers'],config['constraint'])\n",
    "    \n",
    "    #data model \n",
    "    if 'Diffusion' in config['name']:\n",
    "    #data model \n",
    "        data_model = DiffusionDataTrign(config['name'],config['Nt'],\\\n",
    "                                   config['dt'],config['dx'],config['viscosity'],batch_size=config['batch_size'],\\\n",
    "                                  time_scheme=config['data_timescheme'],acc_order=config['acc_order']+1)        \n",
    "    if 'Burgers' in config['name']:\n",
    "        data_model = BurgersEqnTrign(config['name'],config['Nt'],\\\n",
    "                               config['dt'],config['dx'],config['viscosity'],batch_size=config['batch_size'],\\\n",
    "                              time_scheme=config['data_timescheme'],acc_order=config['acc_order'])\n",
    "    #possible some callbacks\n",
    "    callbacks = None\n",
    "    return model,data_model,callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_diff_moment.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Diffusion Eqn Moment',\n",
       " 'dt': 0.01,\n",
       " 'dx': 0.1,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 2,\n",
       " 'acc_order': 1,\n",
       " 'n_hidden_layers': 2,\n",
       " 'dataname': 'Diffusion',\n",
       " 'viscosity': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'channel_names': 'u',\n",
       " 'data_timescheme': 'rk4',\n",
       " 'data_dir': '/sdsd/dsds/sdsd',\n",
       " 'Nt': 100,\n",
       " 'Nx': 32,\n",
       " 'sigma': 1,\n",
       " 'sparsity': 0.005,\n",
       " 'momentsparsity': 0.001,\n",
       " 'epochs': 500,\n",
       " 'results_dir': '/comet/results/',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.005,\n",
       " 'constraint': 'moment'}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = config['blocks']\n",
    "dt = config['dt']\n",
    "dx = config['dx']\n",
    "epochs = config['epochs']\n",
    "lr = config['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n",
      "[PRINT] Epoch: 0, Loss: 7.986, Data Loss: 7.983, Symnet Regularize: 5.548, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 10, Loss: 4.220, Data Loss: 4.217, Symnet Regularize: 5.074, Moment Regularize: 3.271 \n",
      "[PRINT] Epoch: 20, Loss: 2.508, Data Loss: 2.504, Symnet Regularize: 4.723, Moment Regularize: 3.486 \n",
      "[PRINT] Epoch: 30, Loss: 1.407, Data Loss: 1.403, Symnet Regularize: 4.472, Moment Regularize: 3.572 \n",
      "[PRINT] Epoch: 40, Loss: 0.714, Data Loss: 0.710, Symnet Regularize: 4.297, Moment Regularize: 3.604 \n",
      "[PRINT] Epoch: 50, Loss: 0.327, Data Loss: 0.323, Symnet Regularize: 4.163, Moment Regularize: 3.625 \n",
      "[PRINT] Epoch: 60, Loss: 0.136, Data Loss: 0.132, Symnet Regularize: 4.070, Moment Regularize: 3.636 \n",
      "[PRINT] Epoch: 70, Loss: 0.054, Data Loss: 0.050, Symnet Regularize: 4.001, Moment Regularize: 3.636 \n",
      "[PRINT] Epoch: 80, Loss: 0.024, Data Loss: 0.020, Symnet Regularize: 3.942, Moment Regularize: 3.625 \n",
      "[PRINT] Epoch: 90, Loss: 0.014, Data Loss: 0.011, Symnet Regularize: 3.889, Moment Regularize: 3.606 \n",
      "[PRINT] Epoch: 100, Loss: 0.012, Data Loss: 0.008, Symnet Regularize: 3.842, Moment Regularize: 3.580 \n",
      "[PRINT] Epoch: 110, Loss: 0.010, Data Loss: 0.007, Symnet Regularize: 3.803, Moment Regularize: 3.551 \n",
      "[PRINT] Epoch: 120, Loss: 0.009, Data Loss: 0.006, Symnet Regularize: 3.771, Moment Regularize: 3.521 \n",
      "[PRINT] Epoch: 130, Loss: 0.009, Data Loss: 0.005, Symnet Regularize: 3.744, Moment Regularize: 3.507 \n",
      "[PRINT] Epoch: 140, Loss: 0.008, Data Loss: 0.004, Symnet Regularize: 3.722, Moment Regularize: 3.490 \n",
      "[PRINT] Epoch: 150, Loss: 0.007, Data Loss: 0.004, Symnet Regularize: 3.704, Moment Regularize: 3.475 \n",
      "[PRINT] Epoch: 160, Loss: 0.007, Data Loss: 0.003, Symnet Regularize: 3.689, Moment Regularize: 3.459 \n",
      "[PRINT] Epoch: 170, Loss: 0.006, Data Loss: 0.003, Symnet Regularize: 3.675, Moment Regularize: 3.443 \n",
      "[PRINT] Epoch: 180, Loss: 0.006, Data Loss: 0.003, Symnet Regularize: 3.662, Moment Regularize: 3.431 \n",
      "[PRINT] Epoch: 190, Loss: 0.006, Data Loss: 0.002, Symnet Regularize: 3.651, Moment Regularize: 3.423 \n",
      "[PRINT] Epoch: 200, Loss: 0.005, Data Loss: 0.002, Symnet Regularize: 3.641, Moment Regularize: 3.415 \n",
      "[PRINT] Epoch: 210, Loss: 0.005, Data Loss: 0.002, Symnet Regularize: 3.632, Moment Regularize: 3.407 \n",
      "[PRINT] Epoch: 220, Loss: 0.005, Data Loss: 0.002, Symnet Regularize: 3.623, Moment Regularize: 3.399 \n",
      "[PRINT] Epoch: 230, Loss: 0.005, Data Loss: 0.001, Symnet Regularize: 3.616, Moment Regularize: 3.391 \n",
      "[PRINT] Epoch: 240, Loss: 0.005, Data Loss: 0.001, Symnet Regularize: 3.609, Moment Regularize: 3.383 \n",
      "[PRINT] Epoch: 250, Loss: 0.005, Data Loss: 0.001, Symnet Regularize: 3.602, Moment Regularize: 3.376 \n",
      "[PRINT] Epoch: 260, Loss: 0.005, Data Loss: 0.001, Symnet Regularize: 3.596, Moment Regularize: 3.369 \n",
      "[PRINT] Epoch: 270, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.591, Moment Regularize: 3.361 \n",
      "[PRINT] Epoch: 280, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.586, Moment Regularize: 3.353 \n",
      "[PRINT] Epoch: 290, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.581, Moment Regularize: 3.346 \n",
      "[PRINT] Epoch: 300, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.577, Moment Regularize: 3.338 \n",
      "[PRINT] Epoch: 310, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.573, Moment Regularize: 3.330 \n",
      "[PRINT] Epoch: 320, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.569, Moment Regularize: 3.321 \n",
      "[PRINT] Epoch: 330, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.566, Moment Regularize: 3.313 \n",
      "[PRINT] Epoch: 340, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.563, Moment Regularize: 3.305 \n",
      "[PRINT] Epoch: 350, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.561, Moment Regularize: 3.297 \n",
      "[PRINT] Epoch: 360, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.558, Moment Regularize: 3.291 \n",
      "[PRINT] Epoch: 370, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.556, Moment Regularize: 3.284 \n",
      "[PRINT] Epoch: 380, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.554, Moment Regularize: 3.278 \n",
      "[PRINT] Epoch: 390, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.552, Moment Regularize: 3.272 \n",
      "[PRINT] Epoch: 400, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.551, Moment Regularize: 3.266 \n",
      "[PRINT] Epoch: 410, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.549, Moment Regularize: 3.259 \n",
      "[PRINT] Epoch: 420, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.548, Moment Regularize: 3.253 \n",
      "[PRINT] Epoch: 430, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.546, Moment Regularize: 3.246 \n",
      "[PRINT] Epoch: 440, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.545, Moment Regularize: 3.240 \n",
      "[PRINT] Epoch: 450, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.544, Moment Regularize: 3.234 \n",
      "[PRINT] Epoch: 460, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.542, Moment Regularize: 3.228 \n",
      "[PRINT] Epoch: 470, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.541, Moment Regularize: 3.222 \n",
      "[PRINT] Epoch: 480, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.540, Moment Regularize: 3.215 \n",
      "[PRINT] Epoch: 490, Loss: 0.004, Data Loss: 0.001, Symnet Regularize: 3.539, Moment Regularize: 3.209 \n",
      "[PRINT] block: 1\n",
      "[PRINT] Epoch: 0, Loss: 0.022, Data Loss: 0.001, Symnet Regularize: 3.538, Moment Regularize: 3.203 \n",
      "[PRINT] Epoch: 10, Loss: 0.021, Data Loss: 0.001, Symnet Regularize: 3.485, Moment Regularize: 3.198 \n",
      "[PRINT] Epoch: 20, Loss: 0.021, Data Loss: 0.001, Symnet Regularize: 3.386, Moment Regularize: 3.193 \n",
      "[PRINT] Epoch: 30, Loss: 0.020, Data Loss: 0.001, Symnet Regularize: 3.291, Moment Regularize: 3.190 \n",
      "[PRINT] Epoch: 40, Loss: 0.020, Data Loss: 0.001, Symnet Regularize: 3.211, Moment Regularize: 3.187 \n",
      "[PRINT] Epoch: 50, Loss: 0.020, Data Loss: 0.001, Symnet Regularize: 3.127, Moment Regularize: 3.184 \n",
      "[PRINT] Epoch: 60, Loss: 0.019, Data Loss: 0.001, Symnet Regularize: 3.043, Moment Regularize: 3.181 \n",
      "[PRINT] Epoch: 70, Loss: 0.019, Data Loss: 0.001, Symnet Regularize: 2.959, Moment Regularize: 3.178 \n",
      "[PRINT] Epoch: 80, Loss: 0.018, Data Loss: 0.001, Symnet Regularize: 2.897, Moment Regularize: 3.175 \n",
      "[PRINT] Epoch: 90, Loss: 0.018, Data Loss: 0.001, Symnet Regularize: 2.840, Moment Regularize: 3.173 \n",
      "[PRINT] Epoch: 100, Loss: 0.018, Data Loss: 0.001, Symnet Regularize: 2.784, Moment Regularize: 3.170 \n",
      "[PRINT] Epoch: 110, Loss: 0.018, Data Loss: 0.001, Symnet Regularize: 2.727, Moment Regularize: 3.167 \n",
      "[PRINT] Epoch: 120, Loss: 0.017, Data Loss: 0.001, Symnet Regularize: 2.671, Moment Regularize: 3.164 \n",
      "[PRINT] Epoch: 130, Loss: 0.017, Data Loss: 0.001, Symnet Regularize: 2.618, Moment Regularize: 3.161 \n",
      "[PRINT] Epoch: 140, Loss: 0.017, Data Loss: 0.001, Symnet Regularize: 2.566, Moment Regularize: 3.158 \n",
      "[PRINT] Epoch: 150, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.517, Moment Regularize: 3.155 \n",
      "[PRINT] Epoch: 160, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.486, Moment Regularize: 3.151 \n",
      "[PRINT] Epoch: 170, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.455, Moment Regularize: 3.148 \n",
      "[PRINT] Epoch: 180, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.424, Moment Regularize: 3.145 \n",
      "[PRINT] Epoch: 190, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.394, Moment Regularize: 3.142 \n",
      "[PRINT] Epoch: 200, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.363, Moment Regularize: 3.139 \n",
      "[PRINT] Epoch: 210, Loss: 0.016, Data Loss: 0.001, Symnet Regularize: 2.333, Moment Regularize: 3.135 \n",
      "[PRINT] Epoch: 220, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.302, Moment Regularize: 3.132 \n",
      "[PRINT] Epoch: 230, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.270, Moment Regularize: 3.129 \n",
      "[PRINT] Epoch: 240, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.239, Moment Regularize: 3.125 \n",
      "[PRINT] Epoch: 250, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.207, Moment Regularize: 3.122 \n",
      "[PRINT] Epoch: 260, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.175, Moment Regularize: 3.119 \n",
      "[PRINT] Epoch: 270, Loss: 0.015, Data Loss: 0.001, Symnet Regularize: 2.144, Moment Regularize: 3.115 \n",
      "[PRINT] Epoch: 280, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 2.115, Moment Regularize: 3.112 \n",
      "[PRINT] Epoch: 290, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 2.087, Moment Regularize: 3.108 \n",
      "[PRINT] Epoch: 300, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 2.058, Moment Regularize: 3.104 \n",
      "[PRINT] Epoch: 310, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 2.029, Moment Regularize: 3.101 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 320, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 2.000, Moment Regularize: 3.097 \n",
      "[PRINT] Epoch: 330, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 1.971, Moment Regularize: 3.094 \n",
      "[PRINT] Epoch: 340, Loss: 0.014, Data Loss: 0.001, Symnet Regularize: 1.943, Moment Regularize: 3.090 \n",
      "[PRINT] Epoch: 350, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.917, Moment Regularize: 3.086 \n",
      "[PRINT] Epoch: 360, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.891, Moment Regularize: 3.082 \n",
      "[PRINT] Epoch: 370, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.864, Moment Regularize: 3.079 \n",
      "[PRINT] Epoch: 380, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.838, Moment Regularize: 3.075 \n",
      "[PRINT] Epoch: 390, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.811, Moment Regularize: 3.071 \n",
      "[PRINT] Epoch: 400, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.785, Moment Regularize: 3.067 \n",
      "[PRINT] Epoch: 410, Loss: 0.013, Data Loss: 0.001, Symnet Regularize: 1.758, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 420, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.730, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.703, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.675, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.647, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.619, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.592, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.564, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.012, Data Loss: 0.001, Symnet Regularize: 1.537, Moment Regularize: 3.063 \n",
      "[PRINT] block: 2\n",
      "[PRINT] Epoch: 0, Loss: 0.023, Data Loss: 0.002, Symnet Regularize: 1.509, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.023, Data Loss: 0.002, Symnet Regularize: 1.470, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 20, Loss: 0.023, Data Loss: 0.002, Symnet Regularize: 1.425, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.022, Data Loss: 0.002, Symnet Regularize: 1.381, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.022, Data Loss: 0.002, Symnet Regularize: 1.357, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.022, Data Loss: 0.002, Symnet Regularize: 1.331, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.021, Data Loss: 0.002, Symnet Regularize: 1.307, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.021, Data Loss: 0.002, Symnet Regularize: 1.283, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.021, Data Loss: 0.002, Symnet Regularize: 1.259, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.021, Data Loss: 0.002, Symnet Regularize: 1.234, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.020, Data Loss: 0.002, Symnet Regularize: 1.210, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.020, Data Loss: 0.002, Symnet Regularize: 1.185, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 120, Loss: 0.020, Data Loss: 0.002, Symnet Regularize: 1.161, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.020, Data Loss: 0.002, Symnet Regularize: 1.139, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 1.117, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 1.096, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 1.074, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 1.052, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.019, Data Loss: 0.002, Symnet Regularize: 1.030, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.018, Data Loss: 0.002, Symnet Regularize: 1.008, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.018, Data Loss: 0.002, Symnet Regularize: 0.986, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.018, Data Loss: 0.002, Symnet Regularize: 0.964, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 220, Loss: 0.018, Data Loss: 0.002, Symnet Regularize: 0.942, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.017, Data Loss: 0.002, Symnet Regularize: 0.919, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.017, Data Loss: 0.002, Symnet Regularize: 0.896, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.017, Data Loss: 0.002, Symnet Regularize: 0.873, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.017, Data Loss: 0.002, Symnet Regularize: 0.850, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.017, Data Loss: 0.002, Symnet Regularize: 0.827, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.804, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.784, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.771, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.759, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.746, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.734, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.016, Data Loss: 0.002, Symnet Regularize: 0.721, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.708, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.696, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.683, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.670, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.657, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.644, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.015, Data Loss: 0.002, Symnet Regularize: 0.630, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 420, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.617, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.604, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.590, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.577, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.563, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.549, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.535, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.014, Data Loss: 0.002, Symnet Regularize: 0.522, Moment Regularize: 3.063 \n",
      "[PRINT] block: 3\n",
      "[PRINT] Epoch: 0, Loss: 0.021, Data Loss: 0.005, Symnet Regularize: 0.508, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.021, Data Loss: 0.005, Symnet Regularize: 0.490, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 20, Loss: 0.021, Data Loss: 0.005, Symnet Regularize: 0.471, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.021, Data Loss: 0.005, Symnet Regularize: 0.450, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.020, Data Loss: 0.005, Symnet Regularize: 0.428, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.020, Data Loss: 0.005, Symnet Regularize: 0.407, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.020, Data Loss: 0.005, Symnet Regularize: 0.385, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.019, Data Loss: 0.005, Symnet Regularize: 0.363, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.019, Data Loss: 0.005, Symnet Regularize: 0.341, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.019, Data Loss: 0.005, Symnet Regularize: 0.319, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.018, Data Loss: 0.005, Symnet Regularize: 0.296, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.018, Data Loss: 0.005, Symnet Regularize: 0.274, Moment Regularize: 3.063 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 120, Loss: 0.018, Data Loss: 0.005, Symnet Regularize: 0.251, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.017, Data Loss: 0.005, Symnet Regularize: 0.229, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.017, Data Loss: 0.005, Symnet Regularize: 0.206, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.017, Data Loss: 0.005, Symnet Regularize: 0.183, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.016, Data Loss: 0.005, Symnet Regularize: 0.159, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.016, Data Loss: 0.005, Symnet Regularize: 0.137, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.016, Data Loss: 0.005, Symnet Regularize: 0.118, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.102, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 220, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 420, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.015, Data Loss: 0.005, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] block: 4\n",
      "[PRINT] Epoch: 0, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.023, Data Loss: 0.009, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 20, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.023, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 120, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 220, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 420, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.022, Data Loss: 0.008, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] block: 5\n",
      "[PRINT] Epoch: 0, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 20, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 120, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 220, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.031, Data Loss: 0.014, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 420, Loss: 0.031, Data Loss: 0.014, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.031, Data Loss: 0.013, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] block: 6\n",
      "[PRINT] Epoch: 0, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.042, Data Loss: 0.021, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 20, Loss: 0.042, Data Loss: 0.021, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.094, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 120, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 220, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 420, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.042, Data Loss: 0.020, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] block: 7\n",
      "[PRINT] Epoch: 0, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 20, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 30, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 40, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 50, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 60, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 70, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 80, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 90, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 100, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 110, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 120, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 130, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 140, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 150, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 160, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 170, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 180, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 190, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 200, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 210, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 220, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 230, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 240, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 250, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 260, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 270, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 280, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 290, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 300, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 310, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 320, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 330, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 340, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 350, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 360, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 370, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 380, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 390, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 400, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 410, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 420, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 430, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 440, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 450, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 460, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 470, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 480, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 490, Loss: 0.054, Data Loss: 0.029, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] block: 8\n",
      "[PRINT] Epoch: 0, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.063 \n",
      "[PRINT] Epoch: 10, Loss: 0.070, Data Loss: 0.042, Symnet Regularize: 0.095, Moment Regularize: 3.070 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 20, Loss: 0.070, Data Loss: 0.042, Symnet Regularize: 0.094, Moment Regularize: 3.068 \n",
      "[PRINT] Epoch: 30, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.094, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 40, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 50, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 60, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 70, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 80, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 90, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 100, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 110, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 120, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 130, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 140, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 150, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 160, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 170, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 180, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 190, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 200, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 210, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 220, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 230, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 240, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 250, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 260, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 270, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 280, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 290, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 300, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 310, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 320, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 330, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 340, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 350, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 360, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 370, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 380, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 390, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 400, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 410, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 420, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 430, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 440, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 450, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 460, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 470, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 480, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 490, Loss: 0.069, Data Loss: 0.041, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] block: 9\n",
      "[PRINT] Epoch: 0, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 10, Loss: 0.089, Data Loss: 0.058, Symnet Regularize: 0.093, Moment Regularize: 3.068 \n",
      "[PRINT] Epoch: 20, Loss: 0.089, Data Loss: 0.058, Symnet Regularize: 0.094, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 30, Loss: 0.087, Data Loss: 0.055, Symnet Regularize: 0.094, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 40, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 50, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 60, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 70, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 80, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 90, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 100, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 110, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 120, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 130, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 140, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 150, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 160, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 170, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 180, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 190, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 200, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 210, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 220, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 230, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 240, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 250, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 260, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 270, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 280, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 290, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 300, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 310, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 320, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 330, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 340, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 350, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 360, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 370, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 380, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 390, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 400, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 410, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 420, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 430, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 440, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 450, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 460, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 470, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 480, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 490, Loss: 0.086, Data Loss: 0.054, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] block: 10\n",
      "[PRINT] Epoch: 0, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.064 \n",
      "[PRINT] Epoch: 10, Loss: 0.109, Data Loss: 0.074, Symnet Regularize: 0.093, Moment Regularize: 3.076 \n",
      "[PRINT] Epoch: 20, Loss: 0.107, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.067 \n",
      "[PRINT] Epoch: 30, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.066 \n",
      "[PRINT] Epoch: 40, Loss: 0.107, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.066 \n",
      "[PRINT] Epoch: 50, Loss: 0.107, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 60, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 70, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 80, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 90, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 100, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 110, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 120, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 130, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.094, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 140, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 150, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 160, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 170, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 180, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 190, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 200, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 210, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 220, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 230, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 240, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 250, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 260, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 270, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 280, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 290, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 300, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 310, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 320, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 330, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 340, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.094, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 350, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 360, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 370, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 380, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 390, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 400, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 410, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 420, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 430, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 440, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 450, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 460, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 470, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 480, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n",
      "[PRINT] Epoch: 490, Loss: 0.106, Data Loss: 0.071, Symnet Regularize: 0.093, Moment Regularize: 3.065 \n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    u_obs = data_model.data(stepnum+1) #np array of stepnum elements\n",
    "    for epoch in range(epochs):\n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        loss,data_loss,syment_reg,moment_loss = modelLoss(model,u_obs,config,block)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                  % (epoch,loss,\\\n",
    "                      data_loss,syment_reg,\\\n",
    "                      moment_loss))\n",
    "\n",
    "    \n",
    "# put prints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fd0.moment',\n",
       "              tensor([[[ 1.0000e+00,  6.2113e-05,  1.7956e-03, -1.2160e-04, -6.2169e-03]]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd0.m2k._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd0.m2k._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd0.k2m._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd0.k2m._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd1.moment',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00, -4.6305e-05, -7.4664e-06,  1.4773e-04]]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd1.m2k._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd1.m2k._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd1.k2m._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd1.k2m._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd2.moment',\n",
       "              tensor([[[0.0000, 0.0000, 1.0000, 0.0014, 0.0829]]], dtype=torch.float64)),\n",
       "             ('fd2.m2k._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd2.m2k._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('fd2.k2m._M',\n",
       "              tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "                      [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n",
       "                      [ 2.0000,  0.5000,  0.0000,  0.5000,  2.0000],\n",
       "                      [-1.3333, -0.1667,  0.0000,  0.1667,  1.3333],\n",
       "                      [ 0.6667,  0.0417,  0.0000,  0.0417,  0.6667]], dtype=torch.float64)),\n",
       "             ('fd2.k2m._invM',\n",
       "              tensor([[-0.0000e+00,  8.3333e-02, -8.3333e-02, -5.0000e-01,  1.0000e+00],\n",
       "                      [ 0.0000e+00, -6.6667e-01,  1.3333e+00,  1.0000e+00, -4.0000e+00],\n",
       "                      [ 1.0000e+00,  0.0000e+00, -2.5000e+00,  4.4409e-16,  6.0000e+00],\n",
       "                      [ 0.0000e+00,  6.6667e-01,  1.3333e+00, -1.0000e+00, -4.0000e+00],\n",
       "                      [ 0.0000e+00, -8.3333e-02, -8.3333e-02,  5.0000e-01,  1.0000e+00]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('symnet.layer0.weight',\n",
       "              tensor([[-1.1210e-44, -5.4511e-43, -1.8217e-44],\n",
       "                      [-3.8326e-42,  1.0826e-11, -3.6434e-44]])),\n",
       "             ('symnet.layer0.bias', tensor([-1.6816e-44, -1.4013e-45])),\n",
       "             ('symnet.layer1.weight',\n",
       "              tensor([[-3.4892e-43,  3.2437e-07, -2.3822e-44,  2.3822e-44],\n",
       "                      [ 2.0879e-43, -1.6409e-42, -4.2039e-45, -4.2039e-45]])),\n",
       "             ('symnet.layer1.bias', tensor([-2.8026e-45, -7.7071e-44])),\n",
       "             ('symnet.layer_final.weight',\n",
       "              tensor([[-4.7068e-03, -4.1217e-04,  9.7060e-02, -5.6052e-45, -1.4013e-45]])),\n",
       "             ('symnet.layer_final.bias', tensor([0.0017]))])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.0970599 u_{2}$"
      ],
      "text/plain": [
       "0.0970599*u_2"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet.getEquation(calprec=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast2symbol(layer):\n",
    "    weight,bias = layer.weight.data.cpu().numpy(), \\\n",
    "                layer.bias.data.cpu().numpy()\n",
    "    weight,bias = sympy.Matrix(weight),sympy.Matrix(bias)\n",
    "    return weight,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sympychop(o, calprec):\n",
    "    for i in range(o.shape[0]):\n",
    "        cdict = o[i].expand().as_coefficients_dict()  \n",
    "        o_i = 0\n",
    "        for k,v in cdict.items():\n",
    "            if abs(v)>0.1**calprec:\n",
    "                o_i = o_i+k*v\n",
    "        o[i] = o_i\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
