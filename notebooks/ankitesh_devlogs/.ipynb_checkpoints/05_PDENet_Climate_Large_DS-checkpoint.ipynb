{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sympy\n",
    "import torch.nn.functional as F\n",
    "from backend.utils import *\n",
    "import pickle\n",
    "import configparser\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from backend.lbfgsnew import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep a num_channel_recoverable (for tacking LHFLX,SHFLX variables)\n",
    "class ClimateData(DataModel):\n",
    "    def __init__(self,data_name,Nx,Nt,dt,dx,batch_size,channel_names,\\\n",
    "                 non_eqn_depen_channel, non_eqn_channel, data_file,scaling=1,total_points=-1):\n",
    "        '''\n",
    "            This supports the batch option (handy for large datasets)\n",
    "        '''\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        dx = scaling*dx\n",
    "        super(ClimateData,self).__init__(data_name=data_name,Nt=Nt, Nx=Nx, dt=dt, dx=dx)\n",
    "        self.batch_size = batch_size\n",
    "        self.ds = xr.open_dataset(data_file)\n",
    "        if total_points!=-1:\n",
    "            self.ds = self.ds.sel({\"batch_size\":slice(48*400,48*400+total_points)})\n",
    "         \n",
    "        self.channel_names = channel_names\n",
    "        self.non_eqn_depen_channel = non_eqn_depen_channel\n",
    "        self.n_non_eqn_depen = len(non_eqn_depen_channel)\n",
    "        self.diff_dict = {\"TAP\":\"DTV\",\"QAP\":\"VD01\",\"TBP\":\"TPHYSTND\",\"QBP\":\"PHQ\",\"TCRM\":\"TCDTAdiab\",\"QCRM\":\"QCDTAdiab\"}\n",
    "        self.non_eqn_channel = non_eqn_channel\n",
    "        self.n_eqn = len(channel_names)\n",
    "        self.n_non_eqn = len(non_eqn_channel)\n",
    "        total = self.ds.batch_size.size\n",
    "        self.n_batch = total//batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.sub, self.div = self._getGlobalStats()\n",
    "        \n",
    "    \n",
    "    def _getGlobalStats(self):\n",
    "        return self.ds.min(),self.ds.max()-self.ds.min()\n",
    "\n",
    "    def _generateData(self):\n",
    "        start = (336-self.Nt)//2\n",
    "        end = start+self.Nt\n",
    "        \n",
    "        lev = self.scaling*self.ds.lev.values\n",
    "        \n",
    "        U = []\n",
    "        eqn_channels_vars = []\n",
    "        non_eqn_depen_channel_vars = []\n",
    "        non_eqn_channels_vars = []\n",
    "        diff_dict_key_name = list(self.diff_dict.values())\n",
    "        diff_values_vars = []\n",
    "        diff_dict_keys = list(self.diff_dict.keys())\n",
    "\n",
    "        #shape the eqn vars\n",
    "        for var in self.channel_names:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            eqn_channels_vars.append(v)\n",
    "            \n",
    "        for var in self.non_eqn_depen_channel:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            non_eqn_depen_channel_vars.append(v)\n",
    "        \n",
    "        for i,var in enumerate(diff_dict_key_name):\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            if float(self.div[var]) !=0:\n",
    "                v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            diff_values_vars.append(v)\n",
    "            \n",
    "        n_diff_values_vars = len(diff_values_vars)\n",
    "        #shape the non eqn vas\n",
    "        for var in self.non_eqn_channel:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis]\n",
    "            v = np.tile(v,[1,1,self.Nx])\n",
    "            v = v[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            non_eqn_channels_vars.append(v)\n",
    "        \n",
    "        # interpolate the eqn variables\n",
    "        self.lev_tilde_after =  np.linspace(0,self.Lx,num=self.Nx)\n",
    "\n",
    "        eqn_channels_vars_interp = []\n",
    "        non_eqn_depen_channel_vars_interp = []\n",
    "        diff_values_vars_interp = []\n",
    "\n",
    "        for i,v in enumerate(eqn_channels_vars+diff_values_vars+non_eqn_depen_channel_vars):\n",
    "            batch_size = v.shape[1]\n",
    "            v_interp = np.zeros(v.shape[:-1]+(self.Nx,))\n",
    "            \n",
    "            for t in range(self.Nt):\n",
    "                for b in range(batch_size):\n",
    "                    interp = np.interp(self.lev_tilde_after,lev,v[t][b][0])\n",
    "                    v_interp[t][b][0] = interp\n",
    "        \n",
    "            if i<self.n_eqn:\n",
    "                eqn_channels_vars_interp.append(v_interp) \n",
    "                \n",
    "            elif i<self.n_eqn+n_diff_values_vars:\n",
    "                diff_values_vars_interp.append(v_interp)\n",
    "            else:\n",
    "                non_eqn_depen_channel_vars_interp.append(v_interp)\n",
    "                \n",
    "                \n",
    "        \n",
    "        U = np.concatenate(eqn_channels_vars_interp,axis=2)\n",
    "        dict_ = {\"AP\":[],\"BP\":[],\"CRM\":[]}\n",
    "        dict_keys = list(dict_.keys())\n",
    "        for i in range(3):\n",
    "            dict_[dict_keys[i]] = torch.from_numpy(np.concatenate(diff_values_vars_interp[2*i:2*i+2],axis=2)).type(torch.DoubleTensor)\n",
    "\n",
    "        _U_noneqn = np.concatenate(non_eqn_channels_vars,axis=2)\n",
    "        _U_noneqn_dep = np.concatenate(non_eqn_depen_channel_vars_interp,axis=2)\n",
    "        return U,_U_noneqn_dep,_U_noneqn,dict_\n",
    "        \n",
    "\n",
    "    def data(self,step_num):\n",
    "        '''\n",
    "            fetches the next batch\n",
    "        '''\n",
    "\n",
    "        self.batch_start = self.curr_batch*self.batch_size\n",
    "        self.batch_end = self.batch_start + self.batch_size\n",
    "        self.curr_batch += 1\n",
    "        self._data,self._U_noneqn_dep,self._U_noneqn,self.dict_ = self._generateData()\n",
    "        if self.curr_batch >= self.n_batch:\n",
    "            self.curr_batch = 0\n",
    "        return torch.from_numpy(self._data[:step_num]),torch.from_numpy(self._U_noneqn_dep[:step_num]),\\\n",
    "                torch.from_numpy(self._U_noneqn[:step_num]).type(torch.DoubleTensor),\\\n",
    "                self.dict_\n",
    "                    \n",
    "    \n",
    "        \n",
    "    def visualize(self,b,subset=True):\n",
    "        #displays ith batch plot\n",
    "        ##time x batch_size x 1 x x_dim \n",
    "        u = []\n",
    "        v = []\n",
    "        x,t = self._getMeshPoints()\n",
    "        disp_arr = self._data\n",
    "            \n",
    "        for i in range(len(self._data)):\n",
    "            u.append(np.array(disp_arr[i][b][0]).reshape(-1))\n",
    "            v.append(np.array(disp_arr[i][b][1]).reshape(-1))\n",
    "            \n",
    "        u = np.array(u)\n",
    "        v = np.array(v)\n",
    "        \n",
    "        \n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        X,T = np.meshgrid(x,t)        \n",
    "        surf = ax.plot_surface(T, X, u)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"X\")\n",
    "        plt.title(self.channel_names[0])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        X,T = np.meshgrid(x,t)        \n",
    "        surf = ax.plot_surface(T, X, v)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"X\")\n",
    "        plt.title(self.channel_names[1])\n",
    "        plt.show()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modifying to incorporate untrainable/unlearnable equation channel with and without the lev derivative\n",
    "'''\n",
    "\n",
    "class PdeNet(torch.nn.Module):\n",
    "    def __init__(self,dt, dx, kernel_size, max_diff_order, n_channel,channel_names,\n",
    "                 n_non_eqn_channels,non_eqn_channel_names,dependent_channels,\n",
    "                 acc_order=2,n_hidden=2,\\\n",
    "                constraint='free'):\n",
    "        '''\n",
    "        Input:\n",
    "        '''\n",
    "        super(PdeNet, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dt = dt\n",
    "        self.kernel_size = kernel_size\n",
    "        self.max_diff_order = max_diff_order\n",
    "        self.n_channel = n_channel\n",
    "        self.channel_names = channel_names\n",
    "        self.n_non_eqn_channels = n_non_eqn_channels\n",
    "        self.non_eqn_channel_names = non_eqn_channel_names\n",
    "        self.dependent_channels = dependent_channels\n",
    "        self.n_dependent_channels = len(dependent_channels)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.constraint = constraint\n",
    "                        \n",
    "        if not np.iterable(acc_order):\n",
    "            acc_order = [acc_order,]*(self.max_diff_order+1)\n",
    "            \n",
    "        self.acc_order = acc_order\n",
    "        \n",
    "        #conv operation\n",
    "        for i in range(max_diff_order+1):\n",
    "            kernel = FD1D(dx,kernel_size,i,acc_order[i],constraint=constraint)\n",
    "            self.add_module('fd'+str(i), kernel) #finite difference of order\n",
    "            \n",
    "        #symnet \n",
    "        c = channel_names.split(',')\n",
    "        derivative_channels = []\n",
    "        for ch in c+self.dependent_channels:\n",
    "            for k in range(max_diff_order+1):\n",
    "                derivative_channels.append(ch+'_'+str(k))\n",
    "                \n",
    "        '''CHANGED: for climate''' \n",
    "        for ch in non_eqn_channel_names:\n",
    "            derivative_channels.append(ch)\n",
    "        \n",
    "            \n",
    "        self.derivative_channels = derivative_channels \n",
    "        all_symnets = []\n",
    "        for k in range(self.n_channel):\n",
    "            self.add_module(\"symnet_\"+str(k),SymNet(n_hidden,len(derivative_channels), deriv_channel_names=derivative_channels))\n",
    "            all_symnets.append(self.__getattr__('symnet_'+str(k)))\n",
    "        self.all_symnets = all_symnets\n",
    "    \n",
    "    @property\n",
    "    def fds(self):\n",
    "        for i in range(self.max_diff_order+1):\n",
    "            yield self.__getattr__('fd'+str(i))\n",
    "                \n",
    "    def multistep(self,inputs,non_eqn_depe,non_eqn_t,diff_values,step_num):\n",
    "        #pass it throught the kernels then the symmnet to \n",
    "        '''\n",
    "        Takes multistep through the whole PDE Net.\n",
    "        '''\n",
    "        u = inputs\n",
    "        for i in range(step_num):\n",
    "            uadd = self.RightHandItems(u,non_eqn_depe,non_eqn_t)#will take a dt step from u using the network \n",
    "            u = u + self.dt*(uadd+diff_values[0]+diff_values[1]) #only for QBP\n",
    "        return u\n",
    "    \n",
    "    def symNetParams(self):\n",
    "        params = []\n",
    "        for symnet in self.all_symnets:\n",
    "            params += list(symnet.parameters())\n",
    "        return params\n",
    "    \n",
    "    def diffParams(self):\n",
    "        params = []\n",
    "        for fd in self.fds:\n",
    "            params += list(fd.parameters())\n",
    "        return params\n",
    "\n",
    "    def RightHandItems(self,u,non_eqn_depe,non_eqn_t):\n",
    "        \n",
    "        #convolve the u with the derivative kernals to get the different derivatives \n",
    "        #batch_size x n_channels x X_dim\n",
    "        derives = []\n",
    "        u_split = u.split(1,dim=1)\n",
    "        non_eqn_depe_split = non_eqn_depe.split(1,dim=1)\n",
    "        for ch in range(self.n_channel):       \n",
    "            for i in range(self.max_diff_order+1):\n",
    "                fd_obj = self.__getattr__('fd'+str(i))\n",
    "                deriv_channel_ch_order_i = fd_obj(u_split[ch])\n",
    "                derives.append(deriv_channel_ch_order_i)   \n",
    "        \n",
    "        for ch in range(self.n_dependent_channels):\n",
    "            for i in range(self.max_diff_order+1):\n",
    "                fd_obj = self.__getattr__('fd'+str(i))\n",
    "                deriv_channel_ch_order_i = fd_obj(non_eqn_depe_split[ch])\n",
    "                derives.append(deriv_channel_ch_order_i)   \n",
    "                \n",
    "                \n",
    "            \n",
    "        U = torch.cat(derives, dim=1) #batch_size x n_derivatives x X_dim \n",
    "        U = torch.cat([U,non_eqn_t],dim=1)\n",
    "        \n",
    "        #symnet_output = (batch_size x X_dim x n_derivatives)\n",
    "        u_outs = []\n",
    "        for symnet in self.all_symnets:\n",
    "            u_symnet = symnet(U.permute(0,2,1)) #batch_size x X_dim x n_derivatives\n",
    "            u_out = u_symnet.unsqueeze_(1)\n",
    "            u_outs.append(u_out)\n",
    "        u_out = torch.cat(u_outs,axis=1)#only 1 channel as there will only be 1 symnet\n",
    "        return u_out\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs,non_eqn_depe,non_eqn_t,diff_values,step_num):\n",
    "        '''\n",
    "            inputs of shape batch_size x n_channels x X_dim\n",
    "            step_nums = number of dt blocks to calculate the inputs for\n",
    "        '''\n",
    "        return self.multistep(inputs,non_eqn_depe,non_eqn_t,diff_values,step_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global names are all the parameters\n",
    "def modelLoss(model,u_obs,non_eqn_dep_obs,non_eqn_obs,diff_dict,config,block):\n",
    "    '''\n",
    "        Returns the loss value for so that it can be given to an optimizer\n",
    "        Inputs:\n",
    "            u_obs (batch_size x n_channels x X_dim)\n",
    "            blocks is stepnum\n",
    "    '''\n",
    "    sparsity = config['sparsity']\n",
    "    momentsparsity = config['momentsparsity']\n",
    "    \n",
    "    \n",
    "    if block==0: #warmup\n",
    "        sparsity = 0\n",
    "        momentsparsity = 0\n",
    "    step_num = block if block>=1 else 1\n",
    "    dt = config['dt']\n",
    "    data_loss = 0\n",
    "    symnet_loss = symnetRegularizeLoss(model)\n",
    "    moment_loss = momentRegularizeLoss(model)\n",
    "    ut = u_obs[0]\n",
    "    loss_mse = 0\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    for steps in range(1,step_num+1):\n",
    "        non_eqn_t = non_eqn_obs[steps-1]\n",
    "        non_eqn_dep_t = non_eqn_dep_obs[steps-1]\n",
    "        diff_values = [diff_dict['AP'][steps-1][:,-1:,:],diff_dict['CRM'][steps][:,-1:,:]] #for QBP\n",
    "        ut_next_predicted = model(ut,non_eqn_dep_t,non_eqn_t,diff_values,step_num=1) #take one step from this point #only 1 channel(QBP)\n",
    "        loss_mse_t = mse_loss(ut_next_predicted,u_obs[steps])\n",
    "        loss_mse += loss_mse_t\n",
    "        data_loss += (loss_mse_t/dt**2)/step_num\n",
    "        ut = ut_next_predicted\n",
    "        \n",
    "\n",
    "    loss = data_loss+stepnum*sparsity*symnet_loss+stepnum*momentsparsity*moment_loss\n",
    "    if torch.isnan(loss):\n",
    "#         raise \"Loss Nan\"\n",
    "        loss = (torch.ones(1,requires_grad=True)/torch.zeros(1)).to(loss)\n",
    "    return loss,data_loss,symnet_loss,moment_loss,loss_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##modify channel names and length\n",
    "def setenv(config): #return model and datamodel\n",
    "    model = PdeNet(config['dt'],config['dx']*config['scaling'],config['kernel_size'],config['max_diff_order']\\\n",
    "                   ,config['n_channels'],config['channel_names'],\n",
    "                   config['n_non_eqn_channels'],config['non_eqn_channels'],\n",
    "                   config['dependent_channels'],\n",
    "                   config['acc_order'],config['n_hidden_layers'],config['constraint'])\n",
    "    \n",
    "    data_model =  ClimateData(config['dataname'],config['Nx'],config['Nt']\n",
    "                              ,config['dt'],config['dx'],config['batch_size'],config['channel_vars'],\\\n",
    "                              config['dependent_channels'],\n",
    "                              config['non_eqn_channels'],config['data_file'],config['scaling'],config[\"total\"])\n",
    "        \n",
    "    #possible some callbacks\n",
    "    callbacks = None\n",
    "    return model,data_model,callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/config_large_ds_climate.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Climate BP',\n",
       " 'dt': 1800,\n",
       " 'dx': 10,\n",
       " 'scaling': 30,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 2,\n",
       " 'acc_order': 2,\n",
       " 'n_hidden_layers': 2,\n",
       " 'n_channels': 1,\n",
       " 'n_non_eqn_channels': 4,\n",
       " 'dataname': 'Climate BP',\n",
       " 'batch_size': 32,\n",
       " 'total': 10000,\n",
       " 'channel_names': 'QBP',\n",
       " 'channel_vars': ['QBP'],\n",
       " 'non_eqn_channels': ['LHFLX', 'SHFLX', 'PS', 'SOLIN'],\n",
       " 'dependent_channels': ['TBP'],\n",
       " 'data_file': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/data/preprocessed_pde.nc',\n",
       " 'Nt': 240,\n",
       " 'Nx': 100,\n",
       " 'sparsity': 0.005,\n",
       " 'momentsparsity': 0.004,\n",
       " 'epochs': 1000,\n",
       " 'model_dir': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.01,\n",
       " 'constraint': 'moment',\n",
       " 'optimizer': 'Adam'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = config['blocks']\n",
    "dt = config['dt']\n",
    "dx = config['dx']\n",
    "epochs = config['epochs']\n",
    "lr = config['learning_rate']\n",
    "opti = config['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-50a671c86abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_model' is not defined"
     ]
    }
   ],
   "source": [
    "data_model.n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2578, 0.2578, 0.2578,  ..., 0.2578, 0.2578, 0.2578],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7244, 0.7244, 0.7244,  ..., 0.7244, 0.7244, 0.7244],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]],\n",
       "\n",
       "        [[0.2087, 0.2087, 0.2087,  ..., 0.2087, 0.2087, 0.2087],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7247, 0.7247, 0.7247,  ..., 0.7247, 0.7247, 0.7247],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]],\n",
       "\n",
       "        [[0.2229, 0.2229, 0.2229,  ..., 0.2229, 0.2229, 0.2229],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7271, 0.7271, 0.7271,  ..., 0.7271, 0.7271, 0.7271],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.1893, 0.1893, 0.1893,  ..., 0.1893, 0.1893, 0.1893],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7442, 0.7442, 0.7442,  ..., 0.7442, 0.7442, 0.7442],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]],\n",
       "\n",
       "        [[0.2029, 0.2029, 0.2029,  ..., 0.2029, 0.2029, 0.2029],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7478, 0.7478, 0.7478,  ..., 0.7478, 0.7478, 0.7478],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]],\n",
       "\n",
       "        [[0.1904, 0.1904, 0.1904,  ..., 0.1904, 0.1904, 0.1904],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan],\n",
       "         [0.7599, 0.7599, 0.7599,  ..., 0.7599, 0.7599, 0.7599],\n",
       "         [   nan,    nan,    nan,  ...,    nan,    nan,    nan]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model.data(1)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimizer(config):\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if config['optimizer'] == 'LBFGS':\n",
    "        optimizer = LBFGSNew(model.parameters(), history_size=7, max_iter=10, line_search_fn=True,batch_mode=True)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = getOptimizer(config)\n",
    "decayRate = 0.96\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #for every batch\n",
    "        for b in range(data_model.n_batch):\n",
    "            u_obs,non_eqn_dep_t,non_eqn_t,diff_dict = data_model.data(stepnum+1) #np array of stepnum elements \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "            #forward\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                if loss.requires_grad:\n",
    "                        loss.backward()\n",
    "                return loss\n",
    "            def closureTemp():\n",
    "                optimizer.zero_grad()\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                loss.backward()\n",
    "                return loss,data_loss,syment_reg,moment_loss,loss_mse\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            if b%10==0 and data_model.n_batch!=1:\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()         \n",
    "                print(\"[PRINT] Epoch: %d, Batch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                      % (epoch,b,loss,loss_mse,\\\n",
    "                          data_loss,syment_reg,\\\n",
    "                          moment_loss))\n",
    "\n",
    "        if epoch%10==0:\n",
    "            loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                  % (epoch,loss,loss_mse,\\\n",
    "                      data_loss,syment_reg,\\\n",
    "                      moment_loss))\n",
    "            \n",
    "        if epoch%500==0:\n",
    "            name = \"Block_\"+str(block)+\"_Epoch_\"+str(epoch)+\".pth\"\n",
    "            torch.save(model.state_dict(),config['model_dir']+name)\n",
    "            \n",
    "    my_lr_scheduler.step() #this doesn't matter for LBFGS\n",
    "    name = \"Block_\"+str(block)+\"_Epoch_\"+str(epochs)+\".pth\"\n",
    "    torch.save(model.state_dict(),config['model_dir']+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/PDEExp/data/preprocessed_pde.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.ufuncs.isnan(ds.sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
