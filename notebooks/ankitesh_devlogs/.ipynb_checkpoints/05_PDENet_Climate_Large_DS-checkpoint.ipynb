{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sympy\n",
    "import torch.nn.functional as F\n",
    "from backend.utils import *\n",
    "import pickle\n",
    "import configparser\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from backend.lbfgsnew import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep a num_channel_recoverable (for tacking LHFLX,SHFLX variables)\n",
    "class ClimateData(DataModel):\n",
    "    def __init__(self,data_name,Nx,Nt,dt,dx,batch_size,channel_names,\\\n",
    "                 non_eqn_depen_channel, non_eqn_channel, data_file,scaling=1,total_points=-1):\n",
    "        '''\n",
    "            This supports the batch option (handy for large datasets)\n",
    "        '''\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        dx = scaling*dx\n",
    "        super(ClimateData,self).__init__(data_name=data_name,Nt=Nt, Nx=Nx, dt=dt, dx=dx)\n",
    "        self.batch_size = batch_size\n",
    "        self.ds = xr.open_dataset(data_file)\n",
    "        if total_points!=-1:\n",
    "            self.ds = self.ds.sel({\"batch_size\":slice(48*400,48*400+total_points)})\n",
    "         \n",
    "        self.channel_names = channel_names\n",
    "        self.non_eqn_depen_channel = non_eqn_depen_channel\n",
    "        self.n_non_eqn_depen = len(non_eqn_depen_channel)\n",
    "        self.diff_dict = {\"TAP\":\"DTV\",\"QAP\":\"VD01\",\"TBP\":\"TPHYSTND\",\"QBP\":\"PHQ\",\"TCRM\":\"TCDTAdiab\",\"QCRM\":\"QCDTAdiab\"}\n",
    "        self.non_eqn_channel = non_eqn_channel\n",
    "        self.n_eqn = len(channel_names)\n",
    "        self.n_non_eqn = len(non_eqn_channel)\n",
    "        total = self.ds.batch_size.size\n",
    "        self.n_batch = total//batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.sub, self.div = self._getGlobalStats()\n",
    "        \n",
    "    \n",
    "    def _getGlobalStats(self):\n",
    "        return self.ds.min(),self.ds.max()-self.ds.min()\n",
    "\n",
    "    def _generateData(self):\n",
    "        \n",
    "        start = (336-self.Nt)//2\n",
    "        end = start+self.Nt\n",
    "        \n",
    "        lev = self.scaling*self.ds.lev.values\n",
    "        \n",
    "        U = []\n",
    "        eqn_channels_vars = []\n",
    "        non_eqn_depen_channel_vars = []\n",
    "        non_eqn_channels_vars = []\n",
    "        diff_dict_key_name = list(self.diff_dict.values())\n",
    "        diff_values_vars = []\n",
    "        diff_dict_keys = list(self.diff_dict.keys())\n",
    "\n",
    "        #shape the eqn vars\n",
    "        for var in self.channel_names:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            eqn_channels_vars.append(v)\n",
    "            \n",
    "        for var in self.non_eqn_depen_channel:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            non_eqn_depen_channel_vars.append(v)\n",
    "        \n",
    "        for i,var in enumerate(diff_dict_key_name):\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis,:]\n",
    "            if float(self.div[var]) !=0:\n",
    "                v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            diff_values_vars.append(v)\n",
    "            \n",
    "        n_diff_values_vars = len(diff_values_vars)\n",
    "        #shape the non eqn vas\n",
    "        for var in self.non_eqn_channel:\n",
    "            v = self.ds[var][start:end,self.batch_start:self.batch_end].values[:,:,np.newaxis]\n",
    "            v = np.tile(v,[1,1,self.Nx])\n",
    "            v = v[:,:,np.newaxis,:]\n",
    "            v = (v-float(self.sub[var]))/float(self.div[var])\n",
    "            non_eqn_channels_vars.append(v)\n",
    "        \n",
    "        # interpolate the eqn variables\n",
    "        self.lev_tilde_after =  np.linspace(0,self.Lx,num=self.Nx)\n",
    "\n",
    "        eqn_channels_vars_interp = []\n",
    "        non_eqn_depen_channel_vars_interp = []\n",
    "        diff_values_vars_interp = []\n",
    "\n",
    "        for i,v in enumerate(eqn_channels_vars+diff_values_vars+non_eqn_depen_channel_vars):\n",
    "            batch_size = v.shape[1]\n",
    "            v_interp = np.zeros(v.shape[:-1]+(self.Nx,))\n",
    "            \n",
    "            for t in range(self.Nt):\n",
    "                for b in range(batch_size):\n",
    "                    interp = np.interp(self.lev_tilde_after,lev,v[t][b][0])\n",
    "                    v_interp[t][b][0] = interp\n",
    "        \n",
    "            if i<self.n_eqn:\n",
    "                eqn_channels_vars_interp.append(v_interp) \n",
    "                \n",
    "            elif i<self.n_eqn+n_diff_values_vars:\n",
    "                diff_values_vars_interp.append(v_interp)\n",
    "            else:\n",
    "                non_eqn_depen_channel_vars_interp.append(v_interp)\n",
    "                \n",
    "                \n",
    "        \n",
    "        U = np.concatenate(eqn_channels_vars_interp,axis=2)\n",
    "        dict_ = {\"AP\":[],\"BP\":[],\"CRM\":[]}\n",
    "        dict_keys = list(dict_.keys())\n",
    "        for i in range(3):\n",
    "            dict_[dict_keys[i]] = torch.from_numpy(np.concatenate(diff_values_vars_interp[2*i:2*i+2],axis=2)).type(torch.DoubleTensor)\n",
    "\n",
    "        _U_noneqn = np.concatenate(non_eqn_channels_vars,axis=2)\n",
    "        _U_noneqn_dep = np.concatenate(non_eqn_depen_channel_vars_interp,axis=2)\n",
    "        return U,_U_noneqn_dep,_U_noneqn,dict_\n",
    "        \n",
    "\n",
    "    def data(self,step_num):\n",
    "        '''\n",
    "            fetches the next batch\n",
    "        '''\n",
    "        self.batch_start = self.curr_batch*self.batch_size\n",
    "        self.batch_end = self.batch_start + self.batch_size\n",
    "        self.curr_batch += 1\n",
    "        self._data,self._U_noneqn_dep,self._U_noneqn,self.dict_ = self._generateData()\n",
    "        \n",
    "        if self.curr_batch >= self.n_batch:\n",
    "            self.curr_batch = 0\n",
    "        return torch.from_numpy(self._data[:step_num]),torch.from_numpy(self._U_noneqn_dep[:step_num]),\\\n",
    "                torch.from_numpy(self._U_noneqn[:step_num]).type(torch.DoubleTensor),\\\n",
    "                self.dict_\n",
    "                    \n",
    "    \n",
    "        \n",
    "    def visualize(self,b,subset=True):\n",
    "        #displays ith batch plot\n",
    "        ##time x batch_size x 1 x x_dim \n",
    "        u = []\n",
    "        v = []\n",
    "        x,t = self._getMeshPoints()\n",
    "        disp_arr = self._data\n",
    "            \n",
    "        for i in range(len(self._data)):\n",
    "            u.append(np.array(disp_arr[i][b][0]).reshape(-1))\n",
    "            v.append(np.array(disp_arr[i][b][1]).reshape(-1))\n",
    "            \n",
    "        u = np.array(u)\n",
    "        v = np.array(v)\n",
    "        \n",
    "        \n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        X,T = np.meshgrid(x,t)        \n",
    "        surf = ax.plot_surface(T, X, u)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"X\")\n",
    "        plt.title(self.channel_names[0])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        X,T = np.meshgrid(x,t)        \n",
    "        surf = ax.plot_surface(T, X, v)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"X\")\n",
    "        plt.title(self.channel_names[1])\n",
    "        plt.show()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modifying to incorporate untrainable/unlearnable equation channel with and without the lev derivative\n",
    "'''\n",
    "\n",
    "class PdeNet(torch.nn.Module):\n",
    "    def __init__(self,dt, dx, kernel_size, max_diff_order, n_channel,channel_names,\n",
    "                 n_non_eqn_channels,non_eqn_channel_names,dependent_channels,\n",
    "                 acc_order=2,n_hidden=2,\\\n",
    "                constraint='free'):\n",
    "        '''\n",
    "        Input:\n",
    "        '''\n",
    "        super(PdeNet, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dt = dt\n",
    "        self.kernel_size = kernel_size\n",
    "        self.max_diff_order = max_diff_order\n",
    "        self.n_channel = n_channel\n",
    "        self.channel_names = channel_names\n",
    "        self.n_non_eqn_channels = n_non_eqn_channels\n",
    "        self.non_eqn_channel_names = non_eqn_channel_names\n",
    "        self.dependent_channels = dependent_channels\n",
    "        self.n_dependent_channels = len(dependent_channels)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.constraint = constraint\n",
    "                        \n",
    "        if not np.iterable(acc_order):\n",
    "            acc_order = [acc_order,]*(self.max_diff_order+1)\n",
    "            \n",
    "        self.acc_order = acc_order\n",
    "        \n",
    "        #conv operation\n",
    "        for i in range(max_diff_order+1):\n",
    "            kernel = FD1D(dx,kernel_size,i,acc_order[i],constraint=constraint)\n",
    "            self.add_module('fd'+str(i), kernel) #finite difference of order\n",
    "            \n",
    "        #symnet \n",
    "        c = channel_names.split(',')\n",
    "        derivative_channels = []\n",
    "        for ch in c+self.dependent_channels:\n",
    "            for k in range(max_diff_order+1):\n",
    "                derivative_channels.append(ch+'_'+str(k))\n",
    "                \n",
    "        '''CHANGED: for climate''' \n",
    "        for ch in non_eqn_channel_names:\n",
    "            derivative_channels.append(ch)\n",
    "        \n",
    "            \n",
    "        self.derivative_channels = derivative_channels \n",
    "        all_symnets = []\n",
    "        for k in range(self.n_channel):\n",
    "            self.add_module(\"symnet_\"+str(k),SymNet(n_hidden,len(derivative_channels), deriv_channel_names=derivative_channels))\n",
    "            all_symnets.append(self.__getattr__('symnet_'+str(k)))\n",
    "        self.all_symnets = all_symnets\n",
    "    \n",
    "    @property\n",
    "    def fds(self):\n",
    "        for i in range(self.max_diff_order+1):\n",
    "            yield self.__getattr__('fd'+str(i))\n",
    "                \n",
    "    def multistep(self,inputs,non_eqn_depe,non_eqn_t,diff_values,step_num):\n",
    "        #pass it throught the kernels then the symmnet to \n",
    "        '''\n",
    "        Takes multistep through the whole PDE Net.\n",
    "        '''\n",
    "        u = inputs\n",
    "        for i in range(step_num):\n",
    "            uadd = self.RightHandItems(u,non_eqn_depe,non_eqn_t)#will take a dt step from u using the network \n",
    "            u = u + self.dt*(uadd+diff_values[0]+diff_values[1]) #only for QBP\n",
    "        return u\n",
    "    \n",
    "    def symNetParams(self):\n",
    "        params = []\n",
    "        for symnet in self.all_symnets:\n",
    "            params += list(symnet.parameters())\n",
    "        return params\n",
    "    \n",
    "    def diffParams(self):\n",
    "        params = []\n",
    "        for fd in self.fds:\n",
    "            params += list(fd.parameters())\n",
    "        return params\n",
    "\n",
    "    def RightHandItems(self,u,non_eqn_depe,non_eqn_t):\n",
    "        \n",
    "        #convolve the u with the derivative kernals to get the different derivatives \n",
    "        #batch_size x n_channels x X_dim\n",
    "        derives = []\n",
    "        u_split = u.split(1,dim=1)\n",
    "        non_eqn_depe_split = non_eqn_depe.split(1,dim=1)\n",
    "        for ch in range(self.n_channel):       \n",
    "            for i in range(self.max_diff_order+1):\n",
    "                fd_obj = self.__getattr__('fd'+str(i))\n",
    "                deriv_channel_ch_order_i = fd_obj(u_split[ch])\n",
    "                derives.append(deriv_channel_ch_order_i)   \n",
    "        \n",
    "        for ch in range(self.n_dependent_channels):\n",
    "            for i in range(self.max_diff_order+1):\n",
    "                fd_obj = self.__getattr__('fd'+str(i))\n",
    "                deriv_channel_ch_order_i = fd_obj(non_eqn_depe_split[ch])\n",
    "                derives.append(deriv_channel_ch_order_i)   \n",
    "                \n",
    "                \n",
    "            \n",
    "        U = torch.cat(derives, dim=1) #batch_size x n_derivatives x X_dim \n",
    "        U = torch.cat([U,non_eqn_t],dim=1)\n",
    "        \n",
    "        #symnet_output = (batch_size x X_dim x n_derivatives)\n",
    "        u_outs = []\n",
    "        for symnet in self.all_symnets:\n",
    "            u_symnet = symnet(U.permute(0,2,1)) #batch_size x X_dim x n_derivatives\n",
    "            u_out = u_symnet.unsqueeze_(1)\n",
    "            u_outs.append(u_out)\n",
    "        u_out = torch.cat(u_outs,axis=1)#only 1 channel as there will only be 1 symnet\n",
    "        return u_out\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs,non_eqn_depe,non_eqn_t,diff_values,step_num):\n",
    "        '''\n",
    "            inputs of shape batch_size x n_channels x X_dim\n",
    "            step_nums = number of dt blocks to calculate the inputs for\n",
    "        '''\n",
    "        return self.multistep(inputs,non_eqn_depe,non_eqn_t,diff_values,step_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global names are all the parameters\n",
    "def modelLoss(model,u_obs,non_eqn_dep_obs,non_eqn_obs,diff_dict,config,block):\n",
    "    '''\n",
    "        Returns the loss value for so that it can be given to an optimizer\n",
    "        Inputs:\n",
    "            u_obs (batch_size x n_channels x X_dim)\n",
    "            blocks is stepnum\n",
    "    '''\n",
    "    sparsity = config['sparsity']\n",
    "    momentsparsity = config['momentsparsity']\n",
    "    \n",
    "    if block==0: #warmup\n",
    "        sparsity = 0\n",
    "        momentsparsity = 0\n",
    "    step_num = block if block>=1 else 1\n",
    "    dt = config['dt']\n",
    "    data_loss = 0\n",
    "    symnet_loss = symnetRegularizeLoss(model)\n",
    "    moment_loss = momentRegularizeLoss(model)\n",
    "    \n",
    "    ut = u_obs[0]\n",
    "    loss_mse = 0\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    for steps in range(1,step_num+1):\n",
    "        non_eqn_t = non_eqn_obs[steps-1]\n",
    "        non_eqn_dep_t = non_eqn_dep_obs[steps-1]\n",
    "        diff_values = [diff_dict['AP'][steps-1][:,-1:,:],diff_dict['CRM'][steps][:,-1:,:]] #for QBP\n",
    "        ut_next_predicted = model(ut,non_eqn_dep_t,non_eqn_t,diff_values,step_num=1) #take one step from this point #only 1 channel(QBP)\n",
    "        loss_mse_t = mse_loss(ut_next_predicted,u_obs[steps])\n",
    "        loss_mse += loss_mse_t\n",
    "        data_loss += (loss_mse_t/dt**2)/step_num\n",
    "        ut = ut_next_predicted\n",
    "        \n",
    "\n",
    "    loss = data_loss+stepnum*sparsity*symnet_loss+stepnum*momentsparsity*moment_loss\n",
    "    if torch.isnan(loss):\n",
    "        loss = (torch.ones(1,requires_grad=True)/torch.zeros(1)).to(loss)\n",
    "    return loss,data_loss,symnet_loss,moment_loss,loss_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##modify channel names and length\n",
    "def setenv(config): #return model and datamodel\n",
    "    model = PdeNet(config['dt'],config['dx']*config['scaling'],config['kernel_size'],config['max_diff_order']\\\n",
    "                   ,config['n_channels'],config['channel_names'],\n",
    "                   config['n_non_eqn_channels'],config['non_eqn_channels'],\n",
    "                   config['dependent_channels'],\n",
    "                   config['acc_order'],config['n_hidden_layers'],config['constraint'])\n",
    "    \n",
    "    data_model =  ClimateData(config['dataname'],config['Nx'],config['Nt']\n",
    "                              ,config['dt'],config['dx'],config['batch_size'],config['channel_vars'],\\\n",
    "                              config['dependent_channels'],\n",
    "                              config['non_eqn_channels'],config['data_file'],config['scaling'],config[\"total\"])\n",
    "        \n",
    "    #possible some callbacks\n",
    "    callbacks = None\n",
    "    return model,data_model,callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/config_large_ds_climate.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Climate BP',\n",
       " 'dt': 1800,\n",
       " 'dx': 10,\n",
       " 'scaling': 30,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 3,\n",
       " 'acc_order': 2,\n",
       " 'n_hidden_layers': 3,\n",
       " 'n_channels': 1,\n",
       " 'n_non_eqn_channels': 4,\n",
       " 'dataname': 'Climate BP',\n",
       " 'batch_size': 64,\n",
       " 'total': 10000,\n",
       " 'channel_names': 'QBP',\n",
       " 'channel_vars': ['QBP'],\n",
       " 'non_eqn_channels': ['LHFLX', 'SHFLX', 'PS', 'SOLIN'],\n",
       " 'dependent_channels': ['TBP'],\n",
       " 'data_file': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/data/preprocessed_pde_2.nc',\n",
       " 'Nt': 240,\n",
       " 'Nx': 100,\n",
       " 'sparsity': 0.005,\n",
       " 'momentsparsity': 0.004,\n",
       " 'epochs': 3,\n",
       " 'model_dir': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/models/climateQBP2',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.005,\n",
       " 'constraint': 'moment',\n",
       " 'optimizer': 'LBFGS'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = config['blocks']\n",
    "dt = config['dt']\n",
    "dx = config['dx']\n",
    "epochs = config['epochs']\n",
    "lr = config['learning_rate']\n",
    "opti = config['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimizer(config):\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if config['optimizer'] == 'LBFGS':\n",
    "        optimizer = LBFGSNew(model.parameters(), history_size=7, max_iter=10, line_search_fn=True,batch_mode=True)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = getOptimizer(config)\n",
    "decayRate = 0.96\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model.n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428234148/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.000, Mse Loss: 1048.979, Data Loss: 0.000, Symnet Regularize: 10.213, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.000, Mse Loss: 53.412, Data Loss: 0.000, Symnet Regularize: 9.837, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.000, Mse Loss: 53.380, Data Loss: 0.000, Symnet Regularize: 9.836, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.000, Mse Loss: 53.348, Data Loss: 0.000, Symnet Regularize: 9.834, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.000, Mse Loss: 53.345, Data Loss: 0.000, Symnet Regularize: 9.834, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.000, Mse Loss: 34.090, Data Loss: 0.000, Symnet Regularize: 9.857, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.000, Mse Loss: 34.068, Data Loss: 0.000, Symnet Regularize: 9.856, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.000, Mse Loss: 34.066, Data Loss: 0.000, Symnet Regularize: 9.856, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.000, Mse Loss: 34.065, Data Loss: 0.000, Symnet Regularize: 9.856, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.000, Mse Loss: 58.137, Data Loss: 0.000, Symnet Regularize: 9.842, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.000, Mse Loss: 58.089, Data Loss: 0.000, Symnet Regularize: 9.841, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.000, Mse Loss: 58.047, Data Loss: 0.000, Symnet Regularize: 9.841, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.000, Mse Loss: 58.011, Data Loss: 0.000, Symnet Regularize: 9.840, Moment Regularize: 2.985 \n",
      "[PRINT] block: 1\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.018, Mse Loss: 387.811, Data Loss: 0.000, Symnet Regularize: 1.175, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.014, Mse Loss: 73.572, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.014, Mse Loss: 73.572, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.014, Mse Loss: 73.572, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.014, Mse Loss: 73.572, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.014, Mse Loss: 94.558, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.014, Mse Loss: 94.558, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.014, Mse Loss: 94.558, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.014, Mse Loss: 94.558, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.014, Mse Loss: 69.092, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.014, Mse Loss: 69.102, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.014, Mse Loss: 69.102, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.014, Mse Loss: 69.102, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 2.985 \n",
      "[PRINT] block: 2\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.029, Mse Loss: 380.138, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.029, Mse Loss: 381.548, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.029, Mse Loss: 381.548, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.029, Mse Loss: 381.548, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.029, Mse Loss: 381.548, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.029, Mse Loss: 396.620, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.029, Mse Loss: 394.753, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.029, Mse Loss: 394.753, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.029, Mse Loss: 394.736, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.029, Mse Loss: 376.522, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.029, Mse Loss: 379.073, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.029, Mse Loss: 379.073, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.029, Mse Loss: 379.073, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 2.985 \n",
      "[PRINT] block: 3\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.043, Mse Loss: 1069.543, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.043, Mse Loss: 1071.607, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.043, Mse Loss: 1072.292, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.043, Mse Loss: 1071.825, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.043, Mse Loss: 1071.825, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.043, Mse Loss: 1039.842, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.043, Mse Loss: 1039.875, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.043, Mse Loss: 1039.875, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.043, Mse Loss: 1039.875, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.043, Mse Loss: 1090.883, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.043, Mse Loss: 1090.455, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.043, Mse Loss: 1090.455, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.043, Mse Loss: 1090.455, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 2.985 \n",
      "[PRINT] block: 4\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.057, Mse Loss: 2882.221, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.057, Mse Loss: 2914.590, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.057, Mse Loss: 2914.590, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.057, Mse Loss: 2914.590, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.057, Mse Loss: 2914.590, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.057, Mse Loss: 2960.974, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.057, Mse Loss: 2960.183, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.057, Mse Loss: 2960.183, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.057, Mse Loss: 2960.183, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.057, Mse Loss: 2985.895, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.057, Mse Loss: 2986.022, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.057, Mse Loss: 2986.022, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.057, Mse Loss: 2986.022, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 2.985 \n",
      "[PRINT] block: 5\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.071, Mse Loss: 5533.778, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.071, Mse Loss: 5531.688, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.071, Mse Loss: 5531.387, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.071, Mse Loss: 5531.387, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.071, Mse Loss: 5531.387, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.071, Mse Loss: 5639.153, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.071, Mse Loss: 5591.388, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.071, Mse Loss: 5591.388, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.071, Mse Loss: 5591.388, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.071, Mse Loss: 5584.010, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.071, Mse Loss: 5579.802, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.071, Mse Loss: 5579.802, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.071, Mse Loss: 5579.802, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 2.985 \n",
      "[PRINT] block: 6\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.086, Mse Loss: 10811.101, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.086, Mse Loss: 10790.055, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.086, Mse Loss: 10790.055, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.086, Mse Loss: 10790.055, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.086, Mse Loss: 10790.055, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.086, Mse Loss: 10641.051, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.086, Mse Loss: 10615.477, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.086, Mse Loss: 10615.477, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.086, Mse Loss: 10615.477, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.086, Mse Loss: 10829.458, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.086, Mse Loss: 10867.824, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.086, Mse Loss: 10867.824, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.086, Mse Loss: 10867.824, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] block: 7\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.100, Mse Loss: 16815.125, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.100, Mse Loss: 17195.169, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.100, Mse Loss: 17195.169, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.100, Mse Loss: 17195.169, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.100, Mse Loss: 17195.169, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.100, Mse Loss: 18021.155, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.100, Mse Loss: 18021.814, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.100, Mse Loss: 18021.814, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.100, Mse Loss: 18021.814, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.100, Mse Loss: 17344.255, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.100, Mse Loss: 17344.255, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.100, Mse Loss: 17344.255, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.100, Mse Loss: 17344.255, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 2.985 \n",
      "[PRINT] block: 8\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.114, Mse Loss: 26222.083, Data Loss: 0.001, Symnet Regularize: 0.435, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.114, Mse Loss: 26633.572, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.114, Mse Loss: 26633.572, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.114, Mse Loss: 26633.572, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.114, Mse Loss: 26633.084, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.114, Mse Loss: 26149.908, Data Loss: 0.001, Symnet Regularize: 0.435, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.114, Mse Loss: 26069.622, Data Loss: 0.001, Symnet Regularize: 0.435, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.114, Mse Loss: 26069.621, Data Loss: 0.001, Symnet Regularize: 0.435, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.114, Mse Loss: 26069.621, Data Loss: 0.001, Symnet Regularize: 0.435, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.114, Mse Loss: 26177.610, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.114, Mse Loss: 26052.438, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.114, Mse Loss: 26052.438, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.114, Mse Loss: 26052.438, Data Loss: 0.001, Symnet Regularize: 0.434, Moment Regularize: 2.985 \n",
      "[PRINT] block: 9\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.128, Mse Loss: 33626.767, Data Loss: 0.001, Symnet Regularize: 0.433, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.128, Mse Loss: 38239.272, Data Loss: 0.001, Symnet Regularize: 0.429, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.128, Mse Loss: 38262.609, Data Loss: 0.001, Symnet Regularize: 0.429, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.128, Mse Loss: 38262.609, Data Loss: 0.001, Symnet Regularize: 0.429, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 0, Loss: 0.128, Mse Loss: 38262.609, Data Loss: 0.001, Symnet Regularize: 0.429, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.128, Mse Loss: 37497.032, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.128, Mse Loss: 37501.254, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.128, Mse Loss: 37501.254, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.128, Mse Loss: 37501.254, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.128, Mse Loss: 38413.838, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.128, Mse Loss: 38202.344, Data Loss: 0.001, Symnet Regularize: 0.427, Moment Regularize: 2.985 \n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        u_obs,non_eqn_dep_t,non_eqn_t,diff_dict = data_model.data(stepnum+1) #np array of stepnum elements \n",
    "        \n",
    "        #for every batch\n",
    "        for b in range(data_model.n_batch):\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                if loss.requires_grad:\n",
    "                        loss.backward()\n",
    "                return loss\n",
    "            def closureTemp():\n",
    "                optimizer.zero_grad()\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                loss.backward()\n",
    "                return loss,data_loss,syment_reg,moment_loss,loss_mse\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            if b%50==0 and data_model.n_batch!=1:\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()         \n",
    "                print(\"[PRINT] Epoch: %d, Batch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                      % (epoch,b,loss,loss_mse,\\\n",
    "                          data_loss,syment_reg,\\\n",
    "                          moment_loss))\n",
    "\n",
    "        if epoch%10==0:\n",
    "            loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                  % (epoch,loss,loss_mse,\\\n",
    "                      data_loss,syment_reg,\\\n",
    "                      moment_loss))\n",
    "            \n",
    "        if epoch%500==0:\n",
    "            name = \"Block_\"+str(block)+\"_Epoch_\"+str(epoch)+\".pth\"\n",
    "            torch.save(model.state_dict(),config['model_dir']+name)\n",
    "            \n",
    "    my_lr_scheduler.step() #this doesn't matter for LBFGS\n",
    "    name = \"Block_\"+str(block)+\"_Epoch_\"+str(epochs)+\".pth\"\n",
    "    torch.save(model.state_dict(),config['model_dir']+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load('/oasis/scratch/comet/ankitesh/temp_project/PDEExp/Block_8_Epoch_3.pth')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.00321783 LHFLX - 0.00673577 PS - 0.00101162 QBP_{0} - 0.00231678 SHFLX - 0.00305302 SOLIN - 0.00671309 TBP_{0} - 0.432988$"
      ],
      "text/plain": [
       "-0.00321783*LHFLX - 0.00673577*PS - 0.00101162*QBP_0 - 0.00231678*SHFLX - 0.00305302*SOLIN - 0.00671309*TBP_0 - 0.432988"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet_0.getEquation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.00321783 LHFLX - 0.00673577 PS - 0.00101162 QBP_{0} - 0.00231678 SHFLX - 0.00305302 SOLIN - 0.00671309 TBP_{0} - 0.432988$"
      ],
      "text/plain": [
       "-0.00321783*LHFLX - 0.00673577*PS - 0.00101162*QBP_0 - 0.00231678*SHFLX - 0.00305302*SOLIN - 0.00671309*TBP_0 - 0.432988"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet_0.getEquation(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/config_large_ds_climate.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Climate BP',\n",
       " 'dt': 1800,\n",
       " 'dx': 10,\n",
       " 'scaling': 30,\n",
       " 'blocks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'kernel_size': 5,\n",
       " 'max_diff_order': 3,\n",
       " 'acc_order': 2,\n",
       " 'n_hidden_layers': 3,\n",
       " 'n_channels': 1,\n",
       " 'n_non_eqn_channels': 4,\n",
       " 'dataname': 'Climate BP',\n",
       " 'batch_size': 64,\n",
       " 'total': 10000,\n",
       " 'channel_names': 'QBP',\n",
       " 'channel_vars': ['QBP'],\n",
       " 'non_eqn_channels': ['LHFLX', 'SHFLX', 'PS', 'SOLIN'],\n",
       " 'dependent_channels': ['TBP'],\n",
       " 'data_file': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/data/preprocessed_pde_2.nc',\n",
       " 'Nt': 240,\n",
       " 'Nx': 100,\n",
       " 'sparsity': 0.005,\n",
       " 'momentsparsity': 0.004,\n",
       " 'epochs': 3,\n",
       " 'model_dir': '/oasis/scratch/comet/ankitesh/temp_project/PDEExp/models/climateQBP2',\n",
       " 'seed': -1,\n",
       " 'learning_rate': 0.005,\n",
       " 'constraint': 'moment',\n",
       " 'optimizer': 'LBFGS'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = config['blocks']\n",
    "dt = config['dt']\n",
    "dx = config['dx']\n",
    "epochs = config['epochs']\n",
    "lr = config['learning_rate']\n",
    "opti = config['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,data_model,callbacks = setenv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimizer\n",
    "optimizer = getOptimizer(config)\n",
    "decayRate = 0.96\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] block: 0\n",
      "[PRINT] Warmum Stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428234148/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.001, Mse Loss: 2053.815, Data Loss: 0.001, Symnet Regularize: 12.676, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.000, Mse Loss: 54.229, Data Loss: 0.000, Symnet Regularize: 12.071, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.000, Mse Loss: 54.213, Data Loss: 0.000, Symnet Regularize: 12.071, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.000, Mse Loss: 54.199, Data Loss: 0.000, Symnet Regularize: 12.070, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 0, Loss: 0.000, Mse Loss: 54.198, Data Loss: 0.000, Symnet Regularize: 12.070, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.000, Mse Loss: 35.475, Data Loss: 0.000, Symnet Regularize: 12.099, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.000, Mse Loss: 34.251, Data Loss: 0.000, Symnet Regularize: 12.127, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.000, Mse Loss: 34.177, Data Loss: 0.000, Symnet Regularize: 12.127, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.000, Mse Loss: 34.115, Data Loss: 0.000, Symnet Regularize: 12.127, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.000, Mse Loss: 60.392, Data Loss: 0.000, Symnet Regularize: 12.060, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.000, Mse Loss: 60.333, Data Loss: 0.000, Symnet Regularize: 12.062, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.000, Mse Loss: 60.287, Data Loss: 0.000, Symnet Regularize: 12.063, Moment Regularize: 3.983 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.000, Mse Loss: 60.249, Data Loss: 0.000, Symnet Regularize: 12.064, Moment Regularize: 3.983 \n",
      "[PRINT] block: 1\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.027, Mse Loss: 5568.897, Data Loss: 0.002, Symnet Regularize: 1.925, Moment Regularize: 4.001 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.018, Mse Loss: 73.624, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.018, Mse Loss: 73.624, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.018, Mse Loss: 73.624, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.018, Mse Loss: 73.624, Data Loss: 0.000, Symnet Regularize: 0.479, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.018, Mse Loss: 94.531, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.018, Mse Loss: 94.531, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.018, Mse Loss: 94.531, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.018, Mse Loss: 94.531, Data Loss: 0.000, Symnet Regularize: 0.480, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.018, Mse Loss: 69.164, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.018, Mse Loss: 69.105, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.018, Mse Loss: 69.105, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.018, Mse Loss: 69.105, Data Loss: 0.000, Symnet Regularize: 0.478, Moment Regularize: 3.980 \n",
      "[PRINT] block: 2\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.037, Mse Loss: 402.828, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.037, Mse Loss: 379.873, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.037, Mse Loss: 380.012, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.037, Mse Loss: 380.012, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.037, Mse Loss: 380.012, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.037, Mse Loss: 397.019, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.037, Mse Loss: 394.460, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.037, Mse Loss: 394.460, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.037, Mse Loss: 394.460, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.037, Mse Loss: 378.970, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.037, Mse Loss: 378.228, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.037, Mse Loss: 378.152, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.037, Mse Loss: 378.152, Data Loss: 0.000, Symnet Regularize: 0.471, Moment Regularize: 3.980 \n",
      "[PRINT] block: 3\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.055, Mse Loss: 1064.635, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.055, Mse Loss: 1068.582, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.055, Mse Loss: 1068.582, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.055, Mse Loss: 1069.311, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.055, Mse Loss: 1069.311, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.055, Mse Loss: 1049.885, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.055, Mse Loss: 1048.284, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.055, Mse Loss: 1048.284, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.055, Mse Loss: 1045.261, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.055, Mse Loss: 1085.775, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.055, Mse Loss: 1085.473, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.055, Mse Loss: 1085.473, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.055, Mse Loss: 1085.473, Data Loss: 0.000, Symnet Regularize: 0.468, Moment Regularize: 3.980 \n",
      "[PRINT] block: 4\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.073, Mse Loss: 2895.345, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.073, Mse Loss: 2909.866, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.073, Mse Loss: 2909.866, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.073, Mse Loss: 2909.866, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.073, Mse Loss: 2909.866, Data Loss: 0.000, Symnet Regularize: 0.460, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.073, Mse Loss: 2934.362, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.073, Mse Loss: 2977.079, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.073, Mse Loss: 2977.079, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.073, Mse Loss: 2977.079, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.073, Mse Loss: 2972.174, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.073, Mse Loss: 2973.764, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.073, Mse Loss: 2973.764, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.073, Mse Loss: 2973.764, Data Loss: 0.000, Symnet Regularize: 0.459, Moment Regularize: 3.980 \n",
      "[PRINT] block: 5\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.091, Mse Loss: 5527.788, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.091, Mse Loss: 5524.406, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.091, Mse Loss: 5524.406, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.091, Mse Loss: 5524.406, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.091, Mse Loss: 5524.406, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.091, Mse Loss: 5670.100, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.091, Mse Loss: 5681.149, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.091, Mse Loss: 5681.149, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.091, Mse Loss: 5681.149, Data Loss: 0.000, Symnet Regularize: 0.454, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.091, Mse Loss: 5628.780, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.091, Mse Loss: 5589.213, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.091, Mse Loss: 5589.213, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.091, Mse Loss: 5589.213, Data Loss: 0.000, Symnet Regularize: 0.455, Moment Regularize: 3.980 \n",
      "[PRINT] block: 6\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.109, Mse Loss: 10443.625, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.109, Mse Loss: 10788.514, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.109, Mse Loss: 10788.514, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.109, Mse Loss: 10788.514, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.109, Mse Loss: 10788.514, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.109, Mse Loss: 10576.114, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 50, Loss: 0.109, Mse Loss: 10553.380, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 100, Loss: 0.109, Mse Loss: 10553.380, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 150, Loss: 0.109, Mse Loss: 10553.380, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 0, Loss: 0.110, Mse Loss: 10973.520, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 50, Loss: 0.110, Mse Loss: 10973.297, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 100, Loss: 0.110, Mse Loss: 10973.297, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 2, Batch: 150, Loss: 0.110, Mse Loss: 10973.297, Data Loss: 0.001, Symnet Regularize: 0.448, Moment Regularize: 3.980 \n",
      "[PRINT] block: 7\n",
      "[PRINT] Epoch: 0, Batch: 0, Loss: 0.128, Mse Loss: 17005.876, Data Loss: 0.001, Symnet Regularize: 0.445, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 50, Loss: 0.128, Mse Loss: 17253.397, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 100, Loss: 0.128, Mse Loss: 17253.397, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Batch: 150, Loss: 0.128, Mse Loss: 17253.397, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 0, Loss: 0.128, Mse Loss: 17253.397, Data Loss: 0.001, Symnet Regularize: 0.444, Moment Regularize: 3.980 \n",
      "[PRINT] Epoch: 1, Batch: 0, Loss: 0.128, Mse Loss: 18811.875, Data Loss: 0.001, Symnet Regularize: 0.447, Moment Regularize: 3.980 \n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    print('[PRINT] block:',block)\n",
    "    if block==0:\n",
    "        print('[PRINT] Warmum Stage')\n",
    "    stepnum = block if block>=1 else 1\n",
    "    #get the data at this time #shape [block,batch,channel,X_dim]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        u_obs,non_eqn_dep_t,non_eqn_t,diff_dict = data_model.data(stepnum+1) #np array of stepnum elements \n",
    "        \n",
    "        #for every batch\n",
    "        for b in range(data_model.n_batch):\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                if loss.requires_grad:\n",
    "                        loss.backward()\n",
    "                return loss\n",
    "            def closureTemp():\n",
    "                optimizer.zero_grad()\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = modelLoss(model,u_obs,non_eqn_dep_t,non_eqn_t,diff_dict,config,block)\n",
    "                loss.backward()\n",
    "                return loss,data_loss,syment_reg,moment_loss,loss_mse\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            if b%50==0 and data_model.n_batch!=1:\n",
    "                loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()         \n",
    "                print(\"[PRINT] Epoch: %d, Batch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                      % (epoch,b,loss,loss_mse,\\\n",
    "                          data_loss,syment_reg,\\\n",
    "                          moment_loss))\n",
    "\n",
    "        if epoch%10==0:\n",
    "            loss,data_loss,syment_reg,moment_loss,loss_mse = closureTemp()\n",
    "            print(\"[PRINT] Epoch: %d, Loss: %.3f, Mse Loss: %.3f, Data Loss: %.3f, Symnet Regularize: %.3f, Moment Regularize: %.3f \"\\\n",
    "                  % (epoch,loss,loss_mse,\\\n",
    "                      data_loss,syment_reg,\\\n",
    "                      moment_loss))\n",
    "            \n",
    "        if epoch%500==0:\n",
    "            name = \"Block_\"+str(block)+\"_Epoch_\"+str(epoch)+\".pth\"\n",
    "            torch.save(model.state_dict(),config['model_dir']+name)\n",
    "            \n",
    "    my_lr_scheduler.step() #this doesn't matter for LBFGS\n",
    "    name = \"Block_\"+str(block)+\"_Epoch_\"+str(epochs)+\".pth\"\n",
    "    torch.save(model.state_dict(),config['model_dir']+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load('/oasis/scratch/comet/ankitesh/temp_project/PDEExp/models/climateQBP2Block_8_Epoch_3.pth')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.00302268 LHFLX - 0.00692792 PS - 0.00101159 QBP_{0} - 0.00236166 SHFLX - 0.00313141 SOLIN - 0.00692461 TBP_{0} - 0.432108$"
      ],
      "text/plain": [
       "-0.00302268*LHFLX - 0.00692792*PS - 0.00101159*QBP_0 - 0.00236166*SHFLX - 0.00313141*SOLIN - 0.00692461*TBP_0 - 0.432108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet_0.getEquation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.00302268 LHFLX - 0.00692792 PS - 0.00101159 QBP_{0} + 4.43242 \\cdot 10^{-7} QBP_{1} + 2.10358 \\cdot 10^{-6} QBP_{2} + 3.06251 \\cdot 10^{-6} QBP_{3} - 0.00236166 SHFLX - 0.00313141 SOLIN - 0.00692461 TBP_{0} - 5.92693 \\cdot 10^{-6} TBP_{1} - 1.06703 \\cdot 10^{-6} TBP_{2} - 4.35016 \\cdot 10^{-7} TBP_{3} - 0.432108$"
      ],
      "text/plain": [
       "-0.00302268*LHFLX - 0.00692792*PS - 0.00101159*QBP_0 + 4.43242e-7*QBP_1 + 2.10358e-6*QBP_2 + 3.06251e-6*QBP_3 - 0.00236166*SHFLX - 0.00313141*SOLIN - 0.00692461*TBP_0 - 5.92693e-6*TBP_1 - 1.06703e-6*TBP_2 - 4.35016e-7*TBP_3 - 0.432108"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.symnet_0.getEquation(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
